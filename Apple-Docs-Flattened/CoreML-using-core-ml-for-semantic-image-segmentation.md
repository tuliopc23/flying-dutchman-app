Source: https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation

[ Skip Navigation ](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#app-main)
  * [Global Nav Open Menu](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#ac-gn-menustate)[Global Nav Close Menu](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation)
  * [Apple Developer](https://developer.apple.com/)


[ Search Developer Cancel  ](https://developer.apple.com/search/)
  * [Apple Developer](https://developer.apple.com/)
  * [News](https://developer.apple.com/news/)
  * [Discover](https://developer.apple.com/discover/)
  * [Design](https://developer.apple.com/design/)
  * [Develop](https://developer.apple.com/develop/)
  * [Distribute](https://developer.apple.com/distribute/)
  * [Support](https://developer.apple.com/support/)
  * [Account](https://developer.apple.com/account/)
  * [](https://developer.apple.com/search/)


Cancel 
Only search within “Documentation”
### Quick Links
  * [Downloads](https://developer.apple.com/download/)
  * [Documentation](https://developer.apple.com/documentation/)
  * [Sample Code](https://developer.apple.com/documentation/samplecode/)
  * [Videos](https://developer.apple.com/videos/)
  * [Forums](https://developer.apple.com/forums/)

5 Quick Links
[ Documentation ](https://developer.apple.com/documentation)
[ Open Menu ](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation)
  * SwiftLanguage: Swift


[](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation)
## [ Core ML  ](https://developer.apple.com/documentation/coreml)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 54 symbols inside <root> [MLCPUComputeDevice](https://developer.apple.com/documentation/coreml/mlcpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
16 of 54 symbols inside <root> containing 9 symbols[MLModelAsset](https://developer.apple.com/documentation/coreml/mlmodelasset)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
17 of 54 symbols inside <root>
App integration
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
18 of 54 symbols inside <root> [Downloading and Compiling a Model on the User’s Device](https://developer.apple.com/documentation/coreml/downloading-and-compiling-a-model-on-the-user-s-device)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
19 of 54 symbols inside <root> containing 9 symbols[Model Integration Samples](https://developer.apple.com/documentation/coreml/model-integration-samples)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
1 of 9 symbols inside 1345615585 
Tabular data models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
2 of 9 symbols inside 1345615585 [Integrating a Core ML Model into Your App](https://developer.apple.com/documentation/coreml/integrating-a-core-ml-model-into-your-app)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
3 of 9 symbols inside 1345615585 
Image classification models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
4 of 9 symbols inside 1345615585 [Using Core ML for semantic image segmentation](https://developer.apple.com/documentation/coreml/using-core-ml-for-semantic-image-segmentation)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
5 of 9 symbols inside 1345615585 [Classifying Images with Vision and Core ML](https://developer.apple.com/documentation/coreml/classifying-images-with-vision-and-core-ml)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
6 of 9 symbols inside 1345615585 [Detecting human body poses in an image](https://developer.apple.com/documentation/coreml/detecting-human-body-poses-in-an-image)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
7 of 9 symbols inside 1345615585 [Understanding a Dice Roll with Vision and Object Detection](https://developer.apple.com/documentation/coreml/understanding-a-dice-roll-with-vision-and-object-detection)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
8 of 9 symbols inside 1345615585 
Text classification models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
9 of 9 symbols inside 1345615585 [Finding answers to questions in a text document](https://developer.apple.com/documentation/coreml/finding-answers-to-questions-in-a-text-document)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
20 of 54 symbols inside <root>
Model encryption
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
21 of 54 symbols inside <root> [Generating a Model Encryption Key](https://developer.apple.com/documentation/coreml/generating-a-model-encryption-key)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
22 of 54 symbols inside <root> [Encrypting a Model in Your App](https://developer.apple.com/documentation/coreml/encrypting-a-model-in-your-app)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
23 of 54 symbols inside <root>
Compute devices
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
E
24 of 54 symbols inside <root> containing 6 symbols[MLComputeDevice](https://developer.apple.com/documentation/coreml/mlcomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 54 symbols inside <root> [MLCPUComputeDevice](https://developer.apple.com/documentation/coreml/mlcpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
26 of 54 symbols inside <root> containing 2 symbols[MLGPUComputeDevice](https://developer.apple.com/documentation/coreml/mlgpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
27 of 54 symbols inside <root> containing 2 symbols[MLNeuralEngineComputeDevice](https://developer.apple.com/documentation/coreml/mlneuralenginecomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
rP
28 of 54 symbols inside <root> [MLComputeDeviceProtocol](https://developer.apple.com/documentation/coreml/mlcomputedeviceprotocol)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
29 of 54 symbols inside <root>
Compute plan
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
30 of 54 symbols inside <root> containing 13 symbols[MLComputePlan](https://developer.apple.com/documentation/coreml/mlcomputeplan-1w21n)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
E
31 of 54 symbols inside <root> containing 11 symbols[MLModelStructure](https://developer.apple.com/documentation/coreml/mlmodelstructure-swift.enum)
63 items were found. Tab back to navigate through them. 
/ 
Navigator is ready 
  * [ Core ML ](https://developer.apple.com/documentation/coreml)
  * [ Model Integration Samples ](https://developer.apple.com/documentation/coreml/model-integration-samples)
  * [ Using Core ML for semantic image segmentation ](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation)
  *     * [ Model Integration Samples ](https://developer.apple.com/documentation/coreml/model-integration-samples)
    * Using Core ML for semantic image segmentation 


Sample Code
# Using Core ML for semantic image segmentation
Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.
[ Download ](https://docs-assets.developer.apple.com/published/00ad11cf32e4/UsingCoreMLForSemanticImageSegmentation.zip)
iOS 17.0+iPadOS 17.0+macOS 14.0+Xcode 16.0+
## [Overview](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Overview)
Semantic image-segmentation models identify multiple objects on an input image you provide. For each object, the model output provides the precise locations of each pixel that represents it, so that you can do things like visualize the area of an image that corresponds to the object.
This sample code project shows you how to load the model, read the model’s metadata to get label information, and perform offline inference. It also shows how to create a mask — from the image-segment label — and overlay it on the original image.
### [Select the image-segmentation model](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Select-the-image-segmentation-model)
The sample code project includes an 8-bit palletization image-segmentation model — `DETRResnet50SemanticSegmentationF16P8` — in the `models` directory. Replace the model with a different version by dropping the model `.mlpackage` into the `models` folder in Xcode. To download a DETR model using 16-bit, see [Core ML Models](https://developer.apple.com/machine-learning/models/).
Select the model in the Xcode file navigator to view details in an Xcode model preview area. The following image shows the metadata and operations for the 8-bit model:
![A screenshot of the Xcode model preview area that shows details about the model you select in the file navigator area.](https://docs-assets.developer.apple.com/published/a5c6acaff8377b69ec33e067fcc76162/selected-detr-model-details%402x.png)
To use a model with a different name, but using the same semantics, replace the source code references in `ViewModel` with the model class for `Model` and `ModelOutput`:
```
final class ViewModel: ObservableObject {
    typealias Model = DETRResnet50SemanticSegmentationF16P8
    typealias ModelOutput = DETRResnet50SemanticSegmentationF16P8Output
}

```

### [Load the model](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Load-the-model)
How long it takes to load a model depends on many factors, including the size of the model. The sample app loads the segmentation model asynchronously to avoid blocking the calling thread:
```
nonisolated func loadModel() async {
    do {
        let model = try Model()
        let labels = model.model.segmentationLabels
        await didLoadModel(model, labels: labels)
    } catch {
        Task { @MainActor in
            errorMessage = "The model failed to load: \(error.localizedDescription)"
        }
    }
}

```

In the `MainView`, the sample loads the model using the `task` modifier, and displays a progress view to display the load time. The sample uses the `ObservableObject` protocol to observe when the loading is complete:
```
var body: some View {
    VStack(spacing: 20) {
        
        // Other existing view configuration.


        // Display a load message when the sample loads the model.
        if !viewModel.isModelLoaded {
            ProgressView("Loading model...")
        }


        // Display an error message.
        if let message = viewModel.errorMessage, !message.isEmpty {
            Text("Error: \(message)")
        }
    }
    .photosPicker(isPresented: $viewModel.showPhotoPicker, selection: $viewModel.selectedPhoto)
    .task {
        // Load the model asynchronously.
        if loadModel {
            await viewModel.loadModel()
        }
    }



```

### [Read the image-segmentation model metadata](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Read-the-image-segmentation-model-metadata)
The `Core ML` framework uses optional metadata to map segment label values into strings an app reads. The metadata is in JSON format, and consists of two optional lists of strings:
  * A `label` list that contains the user-readable names for each label
  * A `color` list for suggested colors — as hexadecimal RGB codes — an app can use


The following image shows the label metadata in the Xcode model preview area:
![A screenshot of the Xcode model preview area that shows details about the model you select in the file navigator area. It highlights additional metadata that contains model labels — person, bicycle, car, motorcycle, airplane, bus, and so on.](https://docs-assets.developer.apple.com/published/49ea06b95e07841f4a7d38b1bdf9a321/selected-detr-metadata%402x.png)
The sample app reads the segmentation labels from the model metadata by calling the `readSegmentationLabels` method in `MLModel`. For more information about the format `Core ML` uses for metadata, see [Xcode Model Preview Types](https://apple.github.io/coremltools/docs-guides/source/xcode-model-preview-types.html):
```
extension MLModel {
    /// The segmentation labels specified in the metadata.
    var segmentationLabels: [String] {
        if let metadata = modelDescription.metadata[.creatorDefinedKey] as? [String: Any],
           let params = metadata["com.apple.coreml.model.preview.params"] as? String,
           let data = params.data(using: .utf8),
           let parsed = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
           let labels = parsed["labels"] as? [String] {
            return labels
        } else {
            return []
        }
    }
}

```

Because the colors from the model metadata are optional, a common practice is to use a generic set of colors that an app defines. A semantic image-segmentation model can generate a large number of labels, and may need to reuse colors or use very similar colors. To assist people with color blindness, avoid using only color to identify different objects. For more information about using inclusive colors, see [Inclusive color](https://developer.apple.com/design/human-interface-guidelines/color#Inclusive-color).
### [Perform inference](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Perform-inference)
The 8-bit DETR model takes a `CVPixelBuffer` with the type `kCVPixelFormatType_32ARGB` as the input, and generates a `ShapedArray` of `Int32` as output. For both arrays, the model uses fixed dimensions with the size `(448, 448)`. These values are specific to the model, and can be different for another model. To perform inference on the image-segmentation model, the sample app calls the `performInference` method. After inference, the sample app gets the result array from the `semanticPredictionsShapedArray` property:
```
// Resize the input image to the target size.
let resizedImage = await CIImage(cgImage: inputImage.cgImage!).resized(to: targetSize)


// Get the pixel buffer for the image.
let pixelBuffer = context.render(resizedImage, pixelFormat: kCVPixelFormatType_32ARGB)


// Perform inference.
let result = try model.prediction(image: pixelBuffer)


// Get the result.
let resultArray = result.semanticPredictionsShapedArray

```

### [Overlay the result](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Overlay-the-result)
The sample app provides a visualization of model output by overlaying a masked image — representing the selected segment — on top of the original, like the sky in the following image:
![An image that shows an image mask that the sample overlays on top of the original image.](https://docs-assets.developer.apple.com/published/eea7869f82af6ed71cc39ad781494700/masked-image%402x.png)
Based on the selection, the sample app colors the labeled regions by calling the `renderMask` method in `ViewModel`:
```
func renderMask() throws -> CGImage? {
    guard let resultArray else {
        return nil
    }


    // Convert the results to a mask.
    var bitmap = resultArray.scalars.map { labelIndex in
        let label = self.labelNames[Int(labelIndex)]
        if label == selectedLabel {
            return 0xFFFFFF00 as UInt32
        } else {
            return 0x00000000 as UInt32
        }
    }


    // Convert the mask to an image.
    let width = resultArray.shape[1]
    let height = resultArray.shape[0]
    let image = bitmap.withUnsafeMutableBytes { bytes in
        let context = CGContext(
            data: bytes.baseAddress,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: 4 * width,
            space: CGColorSpace(name: CGColorSpace.sRGB)!,
            bitmapInfo: CGBitmapInfo.byteOrder32Little.rawValue | CGImageAlphaInfo.noneSkipLast.rawValue  // RGB0
        )
        return context?.makeImage()
    }


    return image!
}

```

## [See Also](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#see-also)
### [Image classification models](https://developer.apple.com/documentation/CoreML/using-core-ml-for-semantic-image-segmentation#Image-classification-models)
[Classifying Images with Vision and Core ML](https://developer.apple.com/documentation/coreml/classifying-images-with-vision-and-core-ml)
Crop and scale photos using the Vision framework and classify them with a Core ML model.
[Detecting human body poses in an image](https://developer.apple.com/documentation/coreml/detecting-human-body-poses-in-an-image)
Locate people and the stance of their bodies by analyzing an image with a PoseNet model.
[Understanding a Dice Roll with Vision and Object Detection](https://developer.apple.com/documentation/coreml/understanding-a-dice-roll-with-vision-and-object-detection)
Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.
Current page is Using Core ML for semantic image segmentation 
[Apple](https://www.apple.com)
  1. [Developer](https://developer.apple.com/)
  2. [ Documentation ](https://developer.apple.com/documentation/)


###  Platforms 
Toggle Menu 
  * [iOS](https://developer.apple.com/ios/)
  * [iPadOS](https://developer.apple.com/ipados/)
  * [macOS](https://developer.apple.com/macos/)
  * [tvOS](https://developer.apple.com/tvos/)
  * [visionOS](https://developer.apple.com/visionos/)
  * [watchOS](https://developer.apple.com/watchos/)


###  Tools 
Toggle Menu 
  * [Swift](https://developer.apple.com/swift/)
  * [SwiftUI](https://developer.apple.com/swiftui/)
  * [Swift Playground](https://developer.apple.com/swift-playground/)
  * [TestFlight](https://developer.apple.com/testflight/)
  * [Xcode](https://developer.apple.com/xcode/)
  * [Xcode Cloud](https://developer.apple.com/xcode-cloud/)
  * [SF Symbols](https://developer.apple.com/sf-symbols/)


###  Topics & Technologies 
Toggle Menu 
  * [Accessibility](https://developer.apple.com/accessibility/)
  * [Accessories](https://developer.apple.com/accessories/)
  * [App Extension](https://developer.apple.com/app-extensions/)
  * [App Store](https://developer.apple.com/app-store/)
  * [Audio & Video](https://developer.apple.com/audio/)
  * [Augmented Reality](https://developer.apple.com/augmented-reality/)
  * [Design](https://developer.apple.com/design/)
  * [Distribution](https://developer.apple.com/distribute/)
  * [Education](https://developer.apple.com/education/)
  * [Fonts](https://developer.apple.com/fonts/)
  * [Games](https://developer.apple.com/games/)
  * [Health & Fitness](https://developer.apple.com/health-fitness/)
  * [In-App Purchase](https://developer.apple.com/in-app-purchase/)
  * [Localization](https://developer.apple.com/localization/)
  * [Maps & Location](https://developer.apple.com/maps/)
  * [Machine Learning & AI](https://developer.apple.com/machine-learning/)
  * [Open Source](https://opensource.apple.com/)
  * [Security](https://developer.apple.com/security/)
  * [Safari & Web](https://developer.apple.com/safari/)


###  Resources 
Toggle Menu 
  *   * [Documentation](https://developer.apple.com/documentation/)
  * [Tutorials](https://developer.apple.com/learn/)
  * [Downloads](https://developer.apple.com/download/)
  * [Forums](https://developer.apple.com/forums/)
  * [Videos](https://developer.apple.com/videos/)


###  Support 
Toggle Menu 
  * [Support Articles](https://developer.apple.com/support/articles/)
  * [Contact Us](https://developer.apple.com/contact/)
  * [Bug Reporting](https://developer.apple.com/bug-reporting/)
  * [System Status](https://developer.apple.com/system-status/)


###  Account 
Toggle Menu 
  * [Apple Developer](https://developer.apple.com/account/)
  * [App Store Connect](https://appstoreconnect.apple.com/)
  * [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)
  * [Feedback Assistant](https://feedbackassistant.apple.com/)


###  Programs 
Toggle Menu 
  * [Apple Developer Program](https://developer.apple.com/programs/)
  * [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)
  * [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)
  * [MFi Program](https://mfi.apple.com/)
  * [News Partner Program](https://developer.apple.com/programs/news-partner/)
  * [Video Partner Program](https://developer.apple.com/programs/video-partner/)
  * [Security Bounty Program](https://developer.apple.com/security-bounty/)
  * [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)


###  Events 
Toggle Menu 
  * [Meet with Apple](https://developer.apple.com/events/)
  * [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)
  * [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)
  * [Apple Design Awards](https://developer.apple.com/design/awards/)
  * [Apple Developer Academies](https://developer.apple.com/academies/)
  * [WWDC](https://developer.apple.com/wwdc/)


To submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2FCoreML%2Fusing-core-ml-for-semantic-image-segmentation).
Select a color scheme preference
Light
Dark
Auto
Copyright © 2025 [Apple Inc.](https://www.apple.com) All rights reserved. 
[ Terms of Use ](https://www.apple.com/legal/internet-services/terms/site.html)[ Privacy Policy ](https://www.apple.com/legal/privacy/)[ Agreements and Guidelines ](https://developer.apple.com/support/terms/)
