Source: https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files

[ Skip Navigation ](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#app-main)
  * [Global Nav Open Menu](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#ac-gn-menustate)[Global Nav Close Menu](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)
  * [Apple Developer](https://developer.apple.com/)


[ Search Developer Cancel  ](https://developer.apple.com/search/)
  * [Apple Developer](https://developer.apple.com/)
  * [News](https://developer.apple.com/news/)
  * [Discover](https://developer.apple.com/discover/)
  * [Design](https://developer.apple.com/design/)
  * [Develop](https://developer.apple.com/develop/)
  * [Distribute](https://developer.apple.com/distribute/)
  * [Support](https://developer.apple.com/support/)
  * [Account](https://developer.apple.com/account/)
  * [](https://developer.apple.com/search/)


Cancel 
Only search within “Documentation”
### Quick Links
  * [Downloads](https://developer.apple.com/download/)
  * [Documentation](https://developer.apple.com/documentation/)
  * [Sample Code](https://developer.apple.com/documentation/samplecode/)
  * [Videos](https://developer.apple.com/videos/)
  * [Forums](https://developer.apple.com/forums/)

5 Quick Links
[ Documentation ](https://developer.apple.com/documentation)
[ Open Menu ](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)
  * SwiftLanguage:  Swift  Objective-C 
Language: 
    * Swift 
    * [ Objective-C ](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)


[](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)
## [ AVFoundation  ](https://developer.apple.com/documentation/avfoundation)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
1 of 31 symbols inside 480790132 
Media export
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
2 of 31 symbols inside 480790132 [Exporting video to alternative formats](https://developer.apple.com/documentation/avfoundation/exporting-video-to-alternative-formats)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
3 of 31 symbols inside 480790132 containing 50 symbols[AVAssetExportSession](https://developer.apple.com/documentation/avfoundation/avassetexportsession)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
4 of 31 symbols inside 480790132 
Image generation
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
5 of 31 symbols inside 480790132 [Creating images from a video asset](https://developer.apple.com/documentation/avfoundation/creating-images-from-a-video-asset)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
6 of 31 symbols inside 480790132 containing 25 symbols[AVAssetImageGenerator](https://developer.apple.com/documentation/avfoundation/avassetimagegenerator)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
7 of 31 symbols inside 480790132 
Media reading
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
8 of 31 symbols inside 480790132 [Reading multiview 3D video files](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
9 of 31 symbols inside 480790132 containing 24 symbols[AVAssetReader](https://developer.apple.com/documentation/avfoundation/avassetreader)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
10 of 31 symbols inside 480790132 containing 12 symbols[AVAssetReaderOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
11 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderTrackOutput](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
12 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderAudioMixOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
13 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderVideoCompositionOutput](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
14 of 31 symbols inside 480790132 containing 4 symbols[AVAssetReaderSampleReferenceOutput](https://developer.apple.com/documentation/avfoundation/avassetreadersamplereferenceoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
15 of 31 symbols inside 480790132 containing 6 symbols[AVAssetReaderOutputMetadataAdaptor](https://developer.apple.com/documentation/avfoundation/avassetreaderoutputmetadataadaptor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
16 of 31 symbols inside 480790132 
Media writing
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
17 of 31 symbols inside 480790132 [Converting projected video to Apple Projected Media Profile](https://developer.apple.com/documentation/avfoundation/converting-projected-video-to-apple-projected-media-profile)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
18 of 31 symbols inside 480790132 [Converting side-by-side 3D video to multiview HEVC and spatial video](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
19 of 31 symbols inside 480790132 [Writing fragmented MPEG-4 files for HTTP Live Streaming](https://developer.apple.com/documentation/avfoundation/writing-fragmented-mpeg-4-files-for-http-live-streaming)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
20 of 31 symbols inside 480790132 [Creating spatial photos and videos with spatial metadata](https://developer.apple.com/documentation/imageio/creating-spatial-photos-and-videos-with-spatial-metadata)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
21 of 31 symbols inside 480790132 [Tagging media with video color information](https://developer.apple.com/documentation/avfoundation/tagging-media-with-video-color-information)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
22 of 31 symbols inside 480790132 containing 4 symbols[Evaluating an app’s video color](https://developer.apple.com/documentation/avfoundation/evaluating-an-app-s-video-color)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
23 of 31 symbols inside 480790132 containing 12 symbols[AVOutputSettingsAssistant](https://developer.apple.com/documentation/avfoundation/avoutputsettingsassistant)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
24 of 31 symbols inside 480790132 containing 58 symbols[AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 31 symbols inside 480790132 containing 46 symbols[AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput)
62 items were found. Tab back to navigate through them. 
/ 
Navigator is ready 
  * [ AVFoundation ](https://developer.apple.com/documentation/avfoundation)
  * [ Media reading and writing ](https://developer.apple.com/documentation/avfoundation/media-reading-and-writing)
  * [ Reading multiview 3D video files ](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files)
  *     * [ Media reading and writing ](https://developer.apple.com/documentation/avfoundation/media-reading-and-writing)
    * Reading multiview 3D video files 


Sample Code
# Reading multiview 3D video files
Render single images for the left eye and right eye from a multiview High Efficiency Video Coding format file by reading individual video frames.
[ Download ](https://docs-assets.developer.apple.com/published/03b0a926288f/ReadingMultiview3DVideoFiles.zip)
macOS 14.0+Xcode 15.2+
## [Overview](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Overview)
Multiview High Efficiency Video Coding (MV-HEVC) media files contain information to produce stereoscopic frames, one for the left eye and one for the right, to create an effect of depth and allow for 3D video. This is the standard format for presenting 3D video in visionOS, encoded as MPEG-4 or QuickTime files.
Previewing and testing MV-HEVC files without hardware requires the ability to load, view, and step through the video data on a timeline. This sample app opens a media file, checking for the MV-HEVC format, then presents a view containing the individual frames at the timestamp. Step through the timeline by dragging the slider to a specific timestamp, or advance to the next frame by pressing the Space bar.
For the full details of the MV-HEVC format, see [Apple HEVC Stereo Video - Interoperability Profile (PDF)](https://developer.apple.com/av-foundation/HEVC-Stereo-Video-Profile.pdf) and [ISO Base Media File Format and Apple HEVC Stereo Video (PDF)](https://developer.apple.com/av-foundation/Stereo-Video-ISOBMFF-Extensions.pdf).
### [Load and inspect the media asset](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Load-and-inspect-the-media-asset)
The app first displays a button labeled Open MVHEVC File. When selected, the button presents an [`NSOpenPanel`](https://developer.apple.com/documentation/AppKit/NSOpenPanel) for choosing video media. Next, the app initializes a `MediaDetailViewModel`, loading this file as an [`AVURLAsset`](https://developer.apple.com/documentation/avfoundation/avurlasset). Before opening the file to present any elements for a stereo video frame, the app ensures a playable, readable file, and gets its total length in time. This is all performed in the initializer.
```
init(filename: URL) {
    asset = AVURLAsset(url: filename)
    Task { @MainActor in
        do {
            let (duration, isPlayable, isReadable) = try await asset.load(.duration, .isPlayable, .isReadable)
            self.duration = duration.seconds
            self.isPlayable = isPlayable
            self.isReadable = isReadable
        } catch {
            self.error = error
        }
    }
}

```

### [Load track data and timestamps](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Load-track-data-and-timestamps)
After confirming the track is readable video data, the app initializes a `StereoViewModel`by calling [`loadTracks(withMediaCharacteristic:completionHandler:)`](https://developer.apple.com/documentation/avfoundation/avasset/loadtracks\(withmediacharacteristic:completionhandler:\)) requesting a [`containsStereoMultiviewVideo`](https://developer.apple.com/documentation/avfoundation/avmediacharacteristic/containsstereomultiviewvideo) track. This check confirms that the file meets the MV-HEVC specification and has valid stereo data.
```
if let track = try await asset.loadTracks(withMediaCharacteristic: .containsStereoMultiviewVideo).first {
    self.track = track

```

Next, the app pulls available timestamps for each frame in the track by calling `presentationTimesFor(track:asset:)`. The app places a video sample cursor at the start of the track with [`makeSampleCursorAtFirstSampleInDecodeOrder()`](https://developer.apple.com/documentation/avfoundation/avassettrack/makesamplecursoratfirstsampleindecodeorder\(\)), then creates a new [`AVSampleBufferGenerator`](https://developer.apple.com/documentation/avfoundation/avsamplebuffergenerator) and [`AVSampleBufferRequest`](https://developer.apple.com/documentation/avfoundation/avsamplebufferrequest).
```
guard let cursor = track.makeSampleCursorAtFirstSampleInDecodeOrder() else {
    return []
}
let sampleBufferGenerator = AVSampleBufferGenerator(asset: asset, timebase: nil)
var presentationTimes = [CMTime]()
let request = AVSampleBufferRequest(start: cursor)
var numSamples: Int64 = 0

```

To read the timestamps, obtain the sample buffer for the current cursor from [`makeSampleBuffer(for:)`](https://developer.apple.com/documentation/avfoundation/avsamplebuffergenerator/makesamplebuffer\(for:\)), then add the [`presentationTimeStamp`](https://developer.apple.com/documentation/CoreMedia/CMSampleBuffer/presentationTimeStamp) for the frame. The cursor steps forward by calling [`stepInDecodeOrder(byCount:)`](https://developer.apple.com/documentation/avfoundation/avsamplecursor/stepindecodeorder\(bycount:\)), reading and caching timestamps for each frame in the buffer. When `stepInDecodeOrder(byCount:)` returns no next frame, sample times are in the cache and reading the video track completes.
```
repeat {
    let buf = try sampleBufferGenerator.makeSampleBuffer(for: request)
    presentationTimes.append(buf.presentationTimeStamp)
    numSamples = cursor.stepInDecodeOrder(byCount: 1)
} while numSamples == 1

```

### [Load video layer information](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Load-video-layer-information)
After preparing timestamps, the app calls `loadVideoLayerIdsForTrack()` to get the layer IDs for the two tracks associated with the left and right eyes. The app calls [`load(_:isolation:)`](https://developer.apple.com/documentation/avfoundation/avasynchronouskeyvalueloading/load\(_:isolation:\))to retrieve metadata, then filters the layer data out of the first available track’s [`tagCollections`](https://developer.apple.com/documentation/CoreMedia/CMFormatDescription/tagCollections). The filter predicate is [`value(onlyIfMatching:)`](https://developer.apple.com/documentation/CoreMedia/CMTag-swift.class/value\(onlyIfMatching:\)), extracting only video layer IDs.
```
private func loadVideoLayerIdsForTrack(_ videoTrack: AVAssetTrack) async throws -> [Int64]? {
    let formatDescriptions = try await videoTrack.load(.formatDescriptions)
    var tags = [Int64]()
    if let tagCollections = formatDescriptions.first?.tagCollections {
        tags = tagCollections.flatMap({ $0 }).compactMap { tag in
            tag.value(onlyIfMatching: .videoLayerID)
        }
    }
    return tags
}

```

### [Load video frames from buffers](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Load-video-frames-from-buffers)
With the timestamp and left eye and right eye video layers identified, `readBufferFromAsset(at:)` calls the `readNextBufferFromAsset()` method of the app to retrieve and display the frame data. The method starts with a series of `guard` checks to ensure read access to the track, creates a local copy of the sample buffer by calling [`copyNextSampleBuffer()`](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput/copynextsamplebuffer\(\)), and retrieves the tagged video buffers from the track.
```
guard let assetReader, let trackOutput else {
    return
}
guard assetReader.status == .reading else {
    publishState(.error(message: "UNEXPECTED STATUS \(assetReader.status)"))
    return
}
guard let sampleBuffer = trackOutput.copyNextSampleBuffer() else {
    publishState(.error(message: "READING SAMPLE BUFFER, STATUS \(assetReader.status), ERROR \(String(describing: assetReader.error))"))
    return
}
guard let taggedBuffers = sampleBuffer.taggedBuffers else {
    publishState(.error(message: "SAMPLE BUFFER CONTAINS NO TAGGED BUFFERS: \(sampleBuffer)"))
    return
}
guard taggedBuffers.count == 2 else {
    publishState(.error(message: "EXPECTED 2 TAGGED BUFFERS, GOT \(taggedBuffers.count)"))
    return
}

```

The app parses each [`CMTaggedBuffer.Buffer.pixelBuffer(_:)`](https://developer.apple.com/documentation/CoreMedia/CMTaggedBuffer/Buffer-swift.enum/pixelBuffer\(_:\)) from the returned sample buffers into an image for display using [`init(cvPixelBuffer:)`](https://developer.apple.com/documentation/CoreImage/CIImage/init\(cvPixelBuffer:\)). The app creates an [`NSImage`](https://developer.apple.com/documentation/AppKit/NSImage) and sets it to the view content as either `leftEye` or `rightEye` depending on whether the view contains a [`stereoView(_:)`](https://developer.apple.com/documentation/CoreMedia/CMTag-swift.class/stereoView\(_:\)) for the left or right eye.
```
taggedBuffers.forEach { taggedBuffer in
    switch taggedBuffer.buffer {
    case let .pixelBuffer(pixelBuffer):
        let ciimage = CIImage(cvPixelBuffer: pixelBuffer)
        let context: CIContext = CIContext(options: nil)
        let cgImage: CGImage = context.createCGImage(ciimage, from: ciimage.extent)!
        let tags = taggedBuffer.tags
        Task {
            await MainActor.run {
                let nsImage = NSImage(cgImage: cgImage, size: NSSize(width: 320, height: 240))
                if tags.contains(.stereoView(.leftEye)) {
                    leftEye = nsImage
                } else if tags.contains(.stereoView(.rightEye)) {
                    rightEye = nsImage
                }
            }
        }
    case .sampleBuffer(let samp):
        publishState(.error(message: "EXPECTED PIXEL BUFFER, GOT SAMPLE BUFFER \(samp)"))
    @unknown default:
        publishState(.error(message: "EXPECTED PIXEL BUFFER TYPE, GOT \(taggedBuffer.buffer)"))
    }
}

```

## [See Also](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#see-also)
### [Media reading](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files#Media-reading)
[`class AVAssetReader`](https://developer.apple.com/documentation/avfoundation/avassetreader)
An object that reads media data from an asset.
[`class AVAssetReaderOutput`](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput)
An abstract class that defines the interface to read media samples from an asset reader.
[`class AVAssetReaderTrackOutput`](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput)
An object that reads media data from a single track of an asset.
[`class AVAssetReaderAudioMixOutput`](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput)
An object that reads audio samples that result from mixing audio from one or more tracks.
[`class AVAssetReaderVideoCompositionOutput`](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput)
An object that reads composited video frames from one or more tracks of an asset.
[`class AVAssetReaderSampleReferenceOutput`](https://developer.apple.com/documentation/avfoundation/avassetreadersamplereferenceoutput)
An object that reads sample references from an asset track.
[`class AVAssetReaderOutputMetadataAdaptor`](https://developer.apple.com/documentation/avfoundation/avassetreaderoutputmetadataadaptor)
An object that creates timed metadata group objects for an asset track.
Current page is Reading multiview 3D video files 
[Apple](https://www.apple.com)
  1. [Developer](https://developer.apple.com/)
  2. [ Documentation ](https://developer.apple.com/documentation/)


###  Platforms 
Toggle Menu 
  * [iOS](https://developer.apple.com/ios/)
  * [iPadOS](https://developer.apple.com/ipados/)
  * [macOS](https://developer.apple.com/macos/)
  * [tvOS](https://developer.apple.com/tvos/)
  * [visionOS](https://developer.apple.com/visionos/)
  * [watchOS](https://developer.apple.com/watchos/)


###  Tools 
Toggle Menu 
  * [Swift](https://developer.apple.com/swift/)
  * [SwiftUI](https://developer.apple.com/swiftui/)
  * [Swift Playground](https://developer.apple.com/swift-playground/)
  * [TestFlight](https://developer.apple.com/testflight/)
  * [Xcode](https://developer.apple.com/xcode/)
  * [Xcode Cloud](https://developer.apple.com/xcode-cloud/)
  * [SF Symbols](https://developer.apple.com/sf-symbols/)


###  Topics & Technologies 
Toggle Menu 
  * [Accessibility](https://developer.apple.com/accessibility/)
  * [Accessories](https://developer.apple.com/accessories/)
  * [App Extension](https://developer.apple.com/app-extensions/)
  * [App Store](https://developer.apple.com/app-store/)
  * [Audio & Video](https://developer.apple.com/audio/)
  * [Augmented Reality](https://developer.apple.com/augmented-reality/)
  * [Design](https://developer.apple.com/design/)
  * [Distribution](https://developer.apple.com/distribute/)
  * [Education](https://developer.apple.com/education/)
  * [Fonts](https://developer.apple.com/fonts/)
  * [Games](https://developer.apple.com/games/)
  * [Health & Fitness](https://developer.apple.com/health-fitness/)
  * [In-App Purchase](https://developer.apple.com/in-app-purchase/)
  * [Localization](https://developer.apple.com/localization/)
  * [Maps & Location](https://developer.apple.com/maps/)
  * [Machine Learning & AI](https://developer.apple.com/machine-learning/)
  * [Open Source](https://opensource.apple.com/)
  * [Security](https://developer.apple.com/security/)
  * [Safari & Web](https://developer.apple.com/safari/)


###  Resources 
Toggle Menu 
  *   * [Documentation](https://developer.apple.com/documentation/)
  * [Tutorials](https://developer.apple.com/learn/)
  * [Downloads](https://developer.apple.com/download/)
  * [Forums](https://developer.apple.com/forums/)
  * [Videos](https://developer.apple.com/videos/)


###  Support 
Toggle Menu 
  * [Support Articles](https://developer.apple.com/support/articles/)
  * [Contact Us](https://developer.apple.com/contact/)
  * [Bug Reporting](https://developer.apple.com/bug-reporting/)
  * [System Status](https://developer.apple.com/system-status/)


###  Account 
Toggle Menu 
  * [Apple Developer](https://developer.apple.com/account/)
  * [App Store Connect](https://appstoreconnect.apple.com/)
  * [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)
  * [Feedback Assistant](https://feedbackassistant.apple.com/)


###  Programs 
Toggle Menu 
  * [Apple Developer Program](https://developer.apple.com/programs/)
  * [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)
  * [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)
  * [MFi Program](https://mfi.apple.com/)
  * [News Partner Program](https://developer.apple.com/programs/news-partner/)
  * [Video Partner Program](https://developer.apple.com/programs/video-partner/)
  * [Security Bounty Program](https://developer.apple.com/security-bounty/)
  * [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)


###  Events 
Toggle Menu 
  * [Meet with Apple](https://developer.apple.com/events/)
  * [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)
  * [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)
  * [Apple Design Awards](https://developer.apple.com/design/awards/)
  * [Apple Developer Academies](https://developer.apple.com/academies/)
  * [WWDC](https://developer.apple.com/wwdc/)


To submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Favfoundation%2Freading-multiview-3d-video-files).
Select a color scheme preference
Light
Dark
Auto
Copyright © 2025 [Apple Inc.](https://www.apple.com) All rights reserved. 
[ Terms of Use ](https://www.apple.com/legal/internet-services/terms/site.html)[ Privacy Policy ](https://www.apple.com/legal/privacy/)[ Agreements and Guidelines ](https://developer.apple.com/support/terms/)
