Source: https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video

[ Skip Navigation ](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#app-main)
  * [Global Nav Open Menu](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#ac-gn-menustate)[Global Nav Close Menu](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
  * [Apple Developer](https://developer.apple.com/)


[ Search Developer Cancel  ](https://developer.apple.com/search/)
  * [Apple Developer](https://developer.apple.com/)
  * [News](https://developer.apple.com/news/)
  * [Discover](https://developer.apple.com/discover/)
  * [Design](https://developer.apple.com/design/)
  * [Develop](https://developer.apple.com/develop/)
  * [Distribute](https://developer.apple.com/distribute/)
  * [Support](https://developer.apple.com/support/)
  * [Account](https://developer.apple.com/account/)
  * [](https://developer.apple.com/search/)


Cancel 
Only search within “Documentation”
### Quick Links
  * [Downloads](https://developer.apple.com/download/)
  * [Documentation](https://developer.apple.com/documentation/)
  * [Sample Code](https://developer.apple.com/documentation/samplecode/)
  * [Videos](https://developer.apple.com/videos/)
  * [Forums](https://developer.apple.com/forums/)

5 Quick Links
[ Documentation ](https://developer.apple.com/documentation)
[ Open Menu ](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
  * SwiftLanguage:  Swift  Objective-C 
Language: 
    * Swift 
    * [ Objective-C ](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)


[](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
## [ AVFoundation  ](https://developer.apple.com/documentation/avfoundation)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
29 of 31 symbols inside 480790132 containing 5 symbols[AVAssetWriterInputGroup](https://developer.apple.com/documentation/avfoundation/avassetwriterinputgroup)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
11 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderTrackOutput](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
12 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderAudioMixOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
13 of 31 symbols inside 480790132 containing 8 symbols[AVAssetReaderVideoCompositionOutput](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
14 of 31 symbols inside 480790132 containing 4 symbols[AVAssetReaderSampleReferenceOutput](https://developer.apple.com/documentation/avfoundation/avassetreadersamplereferenceoutput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
15 of 31 symbols inside 480790132 containing 6 symbols[AVAssetReaderOutputMetadataAdaptor](https://developer.apple.com/documentation/avfoundation/avassetreaderoutputmetadataadaptor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
16 of 31 symbols inside 480790132 
Media writing
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
17 of 31 symbols inside 480790132 [Converting projected video to Apple Projected Media Profile](https://developer.apple.com/documentation/avfoundation/converting-projected-video-to-apple-projected-media-profile)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
18 of 31 symbols inside 480790132 [Converting side-by-side 3D video to multiview HEVC and spatial video](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
19 of 31 symbols inside 480790132 [Writing fragmented MPEG-4 files for HTTP Live Streaming](https://developer.apple.com/documentation/avfoundation/writing-fragmented-mpeg-4-files-for-http-live-streaming)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
20 of 31 symbols inside 480790132 [Creating spatial photos and videos with spatial metadata](https://developer.apple.com/documentation/imageio/creating-spatial-photos-and-videos-with-spatial-metadata)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
21 of 31 symbols inside 480790132 [Tagging media with video color information](https://developer.apple.com/documentation/avfoundation/tagging-media-with-video-color-information)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
22 of 31 symbols inside 480790132 containing 4 symbols[Evaluating an app’s video color](https://developer.apple.com/documentation/avfoundation/evaluating-an-app-s-video-color)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
23 of 31 symbols inside 480790132 containing 12 symbols[AVOutputSettingsAssistant](https://developer.apple.com/documentation/avfoundation/avoutputsettingsassistant)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
24 of 31 symbols inside 480790132 containing 58 symbols[AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 31 symbols inside 480790132 containing 46 symbols[AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
26 of 31 symbols inside 480790132 containing 9 symbols[AVAssetWriterInputPixelBufferAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
27 of 31 symbols inside 480790132 containing 10 symbols[AVAssetWriterInputTaggedPixelBufferGroupAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputtaggedpixelbuffergroupadaptor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
28 of 31 symbols inside 480790132 containing 6 symbols[AVAssetWriterInputMetadataAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputmetadataadaptor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
29 of 31 symbols inside 480790132 containing 5 symbols[AVAssetWriterInputGroup](https://developer.apple.com/documentation/avfoundation/avassetwriterinputgroup)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
30 of 31 symbols inside 480790132 
Captions
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
31 of 31 symbols inside 480790132 containing 18 symbols[Caption authoring](https://developer.apple.com/documentation/avfoundation/caption-authoring)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
6 of 31 symbols inside <root> containing 8 symbols[Media types and utilities](https://developer.apple.com/documentation/avfoundation/media-types-and-utilities)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
7 of 31 symbols inside <root> containing 78 symbols[Video settings](https://developer.apple.com/documentation/avfoundation/video-settings)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
8 of 31 symbols inside <root> containing 14 symbols[Audio settings](https://developer.apple.com/documentation/avfoundation/audio-settings)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
9 of 31 symbols inside <root>
Playback
62 items were found. Tab back to navigate through them. 
/ 
Navigator is ready 
  * [ AVFoundation ](https://developer.apple.com/documentation/avfoundation)
  * [ Media reading and writing ](https://developer.apple.com/documentation/avfoundation/media-reading-and-writing)
  * [ Converting side-by-side 3D video to multiview HEVC and spatial video ](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video)
  *     * [ Media reading and writing ](https://developer.apple.com/documentation/avfoundation/media-reading-and-writing)
    * Converting side-by-side 3D video to multiview HEVC and spatial video 


Sample Code
# Converting side-by-side 3D video to multiview HEVC and spatial video
Create video content for visionOS by converting an existing 3D HEVC file to a multiview HEVC format, optionally adding spatial metadata to create a spatial video.
[ Download ](https://docs-assets.developer.apple.com/published/4a60632cecb7/ConvertingSideBySide3DVideoToMultiviewHEVCAndSpatialVideo.zip)
macOS 15.0+Xcode 16.0+
## [Overview](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Overview)
In visionOS, 3D video uses the _Multiview High Efficiency Video Encoding_ (MV-HEVC) format, supported by MPEG4 and QuickTime. Unlike other 3D media, MV-HEVC stores a single track containing multiple layers for the video, where the track and layers share a frame size. This track frame size is different from other 3D video types, such as _side-by-side video_. Side-by-side videos use a single track, and place the left- and right-eye images next to each other as part of a single video frame.
To convert side-by-side video to MV-HEVC, you load the source video, extract each frame, and then split the frame horizontally. Then copy the left and right sides of the split frame into the left- and right-eye layers, writing a frame containing both layers to the output.
This sample app demonstrates the process for converting side-by-side video files to MV-HEVC, encoding the output as a QuickTime file. The output is placed in the same directory as the input file, with `_MVHEVC` appended to the original filename.
For videos you capture with a consistent camera configuration, you can optionally add spatial metadata to the output file. _Spatial metadata_ describes properties of the left- and right-eye cameras that captured the stereo scene.
Adding spatial metadata to a stereo MV-HEVC video prompts Apple platforms to consider the video as _spatial_ instead of just stereo, and opts the video into visual treatments on Apple Vision Pro that can minimize common causes of stereo viewing discomfort.
To learn more about when to provide spatial metadata for a stereo MV-HEVC video and the metadata values to provide, see [Creating spatial photos and videos with spatial metadata](https://developer.apple.com/documentation/ImageIO/Creating-spatial-photos-and-videos-with-spatial-metadata).
You can verify this sample’s MV-HEVC output by opening it with the sample project from [Reading multiview 3D video files](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files).
For the full details of the MV-HEVC format, see [Apple HEVC Stereo Video - Interoperability Profile (PDF)](https://developer.apple.com/av-foundation/HEVC-Stereo-Video-Profile.pdf) and [ISO Base Media File Format and Apple HEVC Stereo Video (PDF)](https://developer.apple.com/av-foundation/Stereo-Video-ISOBMFF-Extensions.pdf).
### [Configure the sample code project](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Configure-the-sample-code-project)
The app takes a path to a side-by-side stereo input video file as a single command-line argument. To run the app in Xcode, select Product > Scheme > Edit Scheme (Command-<), and add the path to your file to Arguments Passed On Launch.
To also add spatial metadata to the file, add four additional arguments to Arguments Passed On Launch:
  * `--spatial` (or `-s`) to indicate that you want to include spatial metadata
  * `--baseline` (or `-b`) to provide a baseline in millimeters (for example, `--baseline 64.0` for a 64mm baseline)
  * `--fov` (or `-f`) to provide a horizontal field of view in degrees (for example, `--fov 80.0` for an 80-degree field of view)
  * `--disparityAdjustment` (or `-d`) to provide a disparity adjustment (for example, `--disparityAdjustment 0.02` for a 2% positive disparity shift)


By default, the project’s scheme loads a side-by-side video from the Xcode project folder named `Hummingbird.mov`. This video is a sequence of renders of a 3D scene, showing an animated hummingbird model. By default, the app converts this example video to a stereo MV-HEVC file, without spatial metadata.
To add spatial metadata to the hummingbird video during conversion, choose Product > Scheme > Edit Scheme (Command-<), and select the checkbox to the left of the second row of arguments in the Arguments Passed On Launch field. This enables the following additional arguments: `--spatial --baseline 64.0 --fov 80.0 --disparityAdjustment 0.02`.
The `--spatial` argument tells the app to write spatial metadata to the output video. The virtual cameras used to create these renders were positioned 64mm apart with a horizontal field of view of 80 degrees, and so the value for the `--baseline` argument is `64.0`, and the value of the `--fov` argument is `80.0`.
For this video, a disparity adjustment of +2% of the image width produces a comfortable depth effect when the spatial video is presented in a window on Apple Vision Pro. This 2% disparity adjustment value positions the nearest object in the spatial video — the hummingbird — just behind the front of the window, while still keeping an effective illusion of depth between the hummingbird and the background. The scheme’s arguments express the +2% disparity adjustment with a `--disparityAdjustment` value of `0.02`.
### [Load the side-by-side video](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Load-the-side-by-side-video)
The app starts by loading the side-by-side video, creating an [`AVAssetReader`](https://developer.apple.com/documentation/avfoundation/avassetreader). The app calls [`loadTracks(withMediaCharacteristic:completionHandler:)`](https://developer.apple.com/documentation/avfoundation/avasset/loadtracks\(withmediacharacteristic:completionhandler:\)) to load video tracks, and then selects the first track available as the side-by-side input.
```
let asset = AVURLAsset(url: url)
reader = try AVAssetReader(asset: asset)


// Get the side-by-side video track.
guard let videoTrack = try await asset.loadTracks(withMediaCharacteristic: .visual).first else {
    fatalError("Error loading side-by-side video input")
}

```

The app also stores the frame size for the side-by-side video, and calculates the size of the output frames.
```
sideBySideFrameSize = try await videoTrack.load(.naturalSize)
eyeFrameSize = CGSize(width: sideBySideFrameSize.width / 2, height: sideBySideFrameSize.height)

```

To finish loading the video, the app creates an [`AVAssetReaderTrackOutput`](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput) and then adds this output stream to the `AVAssetReader`.
```
let readerSettings: [String: Any] = [
    kCVPixelBufferIOSurfacePropertiesKey as String: [String: String]()
]
sideBySideTrack = AVAssetReaderTrackOutput(track: videoTrack, outputSettings: readerSettings)


if reader.canAdd(sideBySideTrack) {
    reader.add(sideBySideTrack)
}


if !reader.startReading() {
    fatalError(reader.error?.localizedDescription ?? "Unknown error during track read start")
}

```

When creating the reader track output, the app specifies the file’s pixel format and [IOSurface](https://developer.apple.com/documentation/IOSurface) settings in the `readerSettings` dictionary. The app indicates that output goes to a 32-bit ARGB pixel buffer, using [`kCVPixelBufferPixelFormatTypeKey`](https://developer.apple.com/documentation/CoreVideo/kCVPixelBufferPixelFormatTypeKey) with a value of [`kCVPixelFormatType_32ARGB`](https://developer.apple.com/documentation/CoreVideo/kCVPixelFormatType_32ARGB). The sample app also manages its own pixel buffer allocations, passing an empty array as the value for [`kCVPixelBufferIOSurfacePropertiesKey`](https://developer.apple.com/documentation/CoreVideo/kCVPixelBufferIOSurfacePropertiesKey).
### [Configure the output MV-HEVC file](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Configure-the-output-MV-HEVC-file)
With the reader initialized, the app calls the `async` method `transcodeToMVHEVC(output:spatialMetadata:)` to generate the output file. First, the app creates a new [`AVAssetWriter`](https://developer.apple.com/documentation/avfoundation/avassetwriter) pointing to the video output location, and then configures the necessary information on the output to indicate that the file contains MV-HEVC video.
```
var multiviewCompressionProperties: [CFString: Any] = [
    kVTCompressionPropertyKey_MVHEVCVideoLayerIDs: MVHEVCVideoLayerIDs,
    kVTCompressionPropertyKey_MVHEVCViewIDs: MVHEVCViewIDs,
    kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs: MVHEVCLeftAndRightViewIDs,
    kVTCompressionPropertyKey_HasLeftStereoEyeView: true,
    kVTCompressionPropertyKey_HasRightStereoEyeView: true
]

```

[`kVTCompressionPropertyKey_HasLeftStereoEyeView`](https://developer.apple.com/documentation/VideoToolbox/kVTCompressionPropertyKey_HasLeftStereoEyeView) and [`kVTCompressionPropertyKey_HasRightStereoEyeView`](https://developer.apple.com/documentation/VideoToolbox/kVTCompressionPropertyKey_HasRightStereoEyeView) are `true`, because the output contains a layer for each eye. [`kVTCompressionPropertyKey_MVHEVCVideoLayerIDs`](https://developer.apple.com/documentation/VideoToolbox/kVTCompressionPropertyKey_MVHEVCVideoLayerIDs), [`kVTCompressionPropertyKey_MVHEVCViewIDs`](https://developer.apple.com/documentation/VideoToolbox/kVTCompressionPropertyKey_MVHEVCViewIDs), and [`kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs`](https://developer.apple.com/documentation/VideoToolbox/kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs) define the layer and view IDs to use for multiview HEVC encoding. In the sample app, these are all the same.
The sample app uses `0` for the left eye layer/view ID and `1` for the right eye layer/view ID.
```
let MVHEVCVideoLayerIDs = [0, 1]


// For simplicity, choose view IDs that match the layer IDs.
let MVHEVCViewIDs = [0, 1]


// The first element in this array is the view ID of the left eye.
let MVHEVCLeftAndRightViewIDs = [0, 1]

```

### [Include spatial metadata](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Include-spatial-metadata)
If the person calling this command-line app requested to add spatial metadata to the output file, and provided the necessary spatial metadata, the app converts that metadata to expected units and scales, and adds an additional compression property key for each metadata value. The app also specifies that the input uses a rectilinear projection, to indicate that it has the expected projection for spatial video.
```
if let spatialMetadata {


    let baselineInMicrometers = UInt32(1000.0 * spatialMetadata.baselineInMillimeters)
    let encodedHorizontalFOV = UInt32(1000.0 * spatialMetadata.horizontalFOV)
    let encodedDisparityAdjustment = Int32(10_000.0 * spatialMetadata.disparityAdjustment)


    multiviewCompressionProperties[kVTCompressionPropertyKey_ProjectionKind] = kCMFormatDescriptionProjectionKind_Rectilinear
    multiviewCompressionProperties[kVTCompressionPropertyKey_StereoCameraBaseline] = baselineInMicrometers
    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalFieldOfView] = encodedHorizontalFOV
    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalDisparityAdjustment] = encodedDisparityAdjustment


}

```

### [Configure the MV-HEVC input source](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Configure-the-MV-HEVC-input-source)
The app transcodes video by directly copying pixels from the source frame. Writing track data to a video file requires an [`AVAssetWriterInput`](https://developer.apple.com/documentation/avfoundation/avassetwriterinput). The sample app uses an [`AVAssetWriterInputTaggedPixelBufferGroupAdaptor`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputtaggedpixelbuffergroupadaptor) to provide pixel data from the source, writing to the output.
```
let multiviewSettings: [String: Any] = [
    AVVideoCodecKey: AVVideoCodecType.hevc,
    AVVideoWidthKey: self.eyeFrameSize.width,
    AVVideoHeightKey: self.eyeFrameSize.height,
    AVVideoCompressionPropertiesKey: multiviewCompressionProperties
]


guard multiviewWriter.canApply(outputSettings: multiviewSettings, forMediaType: AVMediaType.video) else {
    fatalError("Error applying output settings")
}


let frameInput = AVAssetWriterInput(mediaType: .video, outputSettings: multiviewSettings)


let sourcePixelAttributes: [String: Any] = [
    kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,
    kCVPixelBufferWidthKey as String: self.sideBySideFrameSize.width,
    kCVPixelBufferHeightKey as String: self.sideBySideFrameSize.height
]


let bufferInputAdapter = AVAssetWriterInputTaggedPixelBufferGroupAdaptor(assetWriterInput: frameInput, sourcePixelBufferAttributes: sourcePixelAttributes)

```

The `AVAssetWriterInput` source uses the same `outputSettings` as `videoWriter`, and the created pixel buffer adapter has the same frame size as the source. The app follows the best practice of calling [`canAdd(_:)`](https://developer.apple.com/documentation/avfoundation/avassetwriter/canadd\(_:\)-6al7j) to check the input adapter compatibility before calling [`add(_:)`](https://developer.apple.com/documentation/avfoundation/avassetwriter/add\(_:\)-4c4d0) to use it as a source.
```
guard multiviewWriter.canAdd(frameInput) else {
    fatalError("Error adding side-by-side video frames as input")
}
multiviewWriter.add(frameInput)

```

### [Process input as it becomes available](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Process-input-as-it-becomes-available)
The app calls [`startWriting()`](https://developer.apple.com/documentation/avfoundation/avassetwriter/startwriting\(\)) and [`startSession(atSourceTime:)`](https://developer.apple.com/documentation/avfoundation/avassetwriter/startsession\(atsourcetime:\)) in sequence to start the video writing process, and then iterates over available frame inputs with [`requestMediaDataWhenReady(on:using:)`](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/requestmediadatawhenready\(on:using:\)).
```
guard multiviewWriter.startWriting() else {
    fatalError("Failed to start writing multiview output file")
}
multiviewWriter.startSession(atSourceTime: CMTime.zero)


// The dispatch queue executes the closure when media reads from the input file are available.
frameInput.requestMediaDataWhenReady(on: DispatchQueue(label: "Multiview HEVC Writer")) {

```

The closure argument of `requestMediaDataWhenReady(on:using:)` runs on the provided [`DispatchQueue`](https://developer.apple.com/documentation/Dispatch/DispatchQueue) when the first data read is available. The closure itself is responsible for managing resources that process the media data, and running a loop to process data efficiently.
### [Create the video frame transfer session and output pixel buffer pool](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Create-the-video-frame-transfer-session-and-output-pixel-buffer-pool)
To perform the data transfer from the source track, the pixel input adapter requires a pixel buffer as a source. The app creates a [`VTPixelTransferSession`](https://developer.apple.com/documentation/VideoToolbox/VTPixelTransferSession) to allow for reading data from the video source, and uses the `AVAssetWriterInputTaggedPixelBufferGroupAdaptor`’s existing pixel buffer pool to allocate pixel buffers for the new multiview eye layers.
```
var session: VTPixelTransferSession? = nil
guard VTPixelTransferSessionCreate(allocator: kCFAllocatorDefault, pixelTransferSessionOut: &session) == noErr, let session else {
    fatalError("Failed to create pixel transfer")
}
guard let pixelBufferPool = bufferInputAdapter.pixelBufferPool else {
    fatalError("Failed to retrieve existing pixel buffer pool")
}

```

### [Copy frame images from input to output](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Copy-frame-images-from-input-to-output)
After preparing resources, the app then begins a loop to process frames until there’s no more data, or the input read has stopped to buffer data. The [`isReadyForMoreMediaData`](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/isreadyformoremediadata) property of an input source is `true` if another frame is immediately available to process. When a frame is ready, a [`CVImageBuffer`](https://developer.apple.com/documentation/CoreVideo/CVImageBuffer) instance is created from it.
The app is now ready to handle sampling. If there’s an available sample, the app processes it in the `convertFrame` method, then calls [`appendTaggedBuffers(_:withPresentationTime:)`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputtaggedpixelbuffergroupadaptor/appendtaggedbuffers\(_:withpresentationtime:\)), copying the side-by-side sample buffer’s [`outputPresentationTimeStamp`](https://developer.apple.com/documentation/CoreMedia/CMSampleBuffer/outputPresentationTimeStamp) timestamp to the new multiview timestamp.
```
while frameInput.isReadyForMoreMediaData && bufferInputAdapter.assetWriterInput.isReadyForMoreMediaData {
    if let sampleBuffer = self.sideBySideTrack.copyNextSampleBuffer() {
        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            fatalError("Failed to load source samples as an image buffer")
        }
        let taggedBuffers = self.convertFrame(fromSideBySide: imageBuffer, with: pixelBufferPool, in: session)
        let newPTS = sampleBuffer.outputPresentationTimeStamp
        if !bufferInputAdapter.appendTaggedBuffers(taggedBuffers, withPresentationTime: newPTS) {
            fatalError("Failed to append tagged buffers to multiview output")
        }

```

Input reading finishes when there are no more sample buffers to process from the input stream. The app calls [`markAsFinished()`](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/markasfinished\(\)) to close the stream, and [`finishWriting(completionHandler:)`](https://developer.apple.com/documentation/avfoundation/avassetwriter/finishwriting\(completionhandler:\)) to complete the multiview video write. The app also calls [`resume()`](https://developer.apple.com/documentation/Swift/CheckedContinuation/resume\(\)) on its associated [`CheckedContinuation`](https://developer.apple.com/documentation/Swift/CheckedContinuation), to return to the `await` call, then breaks from the processing loop.
```
frameInput.markAsFinished()
multiviewWriter.finishWriting {
    continuation.resume()
}


break

```

### [Convert side-by-side inputs into video layer outputs](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Convert-side-by-side-inputs-into-video-layer-outputs)
In the `convertFrame` method, the app processes the left- and right-eye images for the frame by `layerID`, using `0` for the left eye and `1` for the right. First, the app creates a pixel buffer from the pool.
```
var pixelBuffer: CVPixelBuffer?
CVPixelBufferPoolCreatePixelBuffer(kCFAllocatorDefault, pixelBufferPool, &pixelBuffer)
guard let pixelBuffer else {
    fatalError("Failed to create pixel buffer for layer \(layerID)")
}

```

The method then uses its passed `VTPixelTransferSession` to copy the pixels from the side-by-side source, placing them into the created output sample buffer by cropping the frame to include only one eye’s image.
```
// Crop the transfer region to the current eye.
let apertureOffset = -(self.eyeFrameSize.width / 2) + CGFloat(layerID) * self.eyeFrameSize.width
let cropRectDict = [
    kCVImageBufferCleanApertureHorizontalOffsetKey: apertureOffset,
    kCVImageBufferCleanApertureVerticalOffsetKey: 0,
    kCVImageBufferCleanApertureWidthKey: self.eyeFrameSize.width,
    kCVImageBufferCleanApertureHeightKey: self.eyeFrameSize.height
]
CVBufferSetAttachment(imageBuffer, kCVImageBufferCleanApertureKey, cropRectDict as CFDictionary, CVAttachmentMode.shouldPropagate)
VTSessionSetProperty(session, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)


// Transfer the image to the pixel buffer.
guard VTPixelTransferSessionTransferImage(session, from: imageBuffer, to: pixelBuffer) == noErr else {
    fatalError("Error during pixel transfer session for layer \(layerID)")
}

```

Setting aperture view properties on [`CVBufferSetAttachment(_:_:_:_:)`](https://developer.apple.com/documentation/CoreVideo/CVBufferSetAttachment\(_:_:_:_:\)) defines how to capture and crop input images. The aperture here is the size of an eye image, and the center of the capture frame offset with [`kCVImageBufferCleanApertureHorizontalOffsetKey`](https://developer.apple.com/documentation/CoreVideo/kCVImageBufferCleanApertureHorizontalOffsetKey) by `-0.5 * width` for the left eye and `+0.5 * width` for the right eye, to capture the correct half of the side-by-side frame.
The app then calls [`VTSessionSetProperty(_:key:value:)`](https://developer.apple.com/documentation/VideoToolbox/VTSessionSetProperty\(_:key:value:\)) to crop the image to the aperture frame with [`kVTScalingMode_CropSourceToCleanAperture`](https://developer.apple.com/documentation/VideoToolbox/kVTScalingMode_CropSourceToCleanAperture). Next, the app calls [`VTPixelTransferSessionTransferImage(_:from:to:)`](https://developer.apple.com/documentation/VideoToolbox/VTPixelTransferSessionTransferImage\(_:from:to:\)) to copy source pixels to the destination buffer.
The final step is to create a [`CMTaggedBuffer`](https://developer.apple.com/documentation/CoreMedia/CMTaggedBuffer) for the eye image to return to the calling output writer.
```
let tags: [CMTag] = [.videoLayerID(Int64(layerID)), .stereoView(eye)]
let buffer = CMTaggedBuffer(tags: tags, buffer: .pixelBuffer(pixelBuffer))
taggedBuffers.append(buffer)

```

## [See Also](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#see-also)
### [Media writing](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video#Media-writing)
[Converting projected video to Apple Projected Media Profile](https://developer.apple.com/documentation/avfoundation/converting-projected-video-to-apple-projected-media-profile)
Convert content with equirectangular or half-equirectangular projection to APMP.
[Writing fragmented MPEG-4 files for HTTP Live Streaming](https://developer.apple.com/documentation/avfoundation/writing-fragmented-mpeg-4-files-for-http-live-streaming)
Create an HTTP Live Streaming presentation by turning a movie file into a sequence of fragmented MPEG-4 files.
[Creating spatial photos and videos with spatial metadata](https://developer.apple.com/documentation/ImageIO/Creating-spatial-photos-and-videos-with-spatial-metadata)
Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.
[Tagging media with video color information](https://developer.apple.com/documentation/avfoundation/tagging-media-with-video-color-information)
Inspect and set video color space information when writing and transcoding media.
[API Reference Evaluating an app’s video color](https://developer.apple.com/documentation/avfoundation/evaluating-an-app-s-video-color)
Check color reproduction for a video in your app by using test patterns, video test equipment, and light-measurement instruments.
[`class AVOutputSettingsAssistant`](https://developer.apple.com/documentation/avfoundation/avoutputsettingsassistant)
An object that builds audio and video output settings dictionaries.
[`class AVAssetWriter`](https://developer.apple.com/documentation/avfoundation/avassetwriter)
An object that writes media data to a container file.
[`class AVAssetWriterInput`](https://developer.apple.com/documentation/avfoundation/avassetwriterinput)
An object that appends media samples to a track in an asset writer’s output file.
[`class AVAssetWriterInputPixelBufferAdaptor`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor)
An object that appends video samples to an asset writer input.
[`class AVAssetWriterInputTaggedPixelBufferGroupAdaptor`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputtaggedpixelbuffergroupadaptor)
An object that appends tagged buffer groups to an asset writer input.
[`class AVAssetWriterInputMetadataAdaptor`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputmetadataadaptor)
An object that appends timed metadata groups to an asset writer input.
[`class AVAssetWriterInputGroup`](https://developer.apple.com/documentation/avfoundation/avassetwriterinputgroup)
A group of inputs with tracks that are mutually exclusive to each other for playback or processing.
Current page is Converting side-by-side 3D video to multiview HEVC and spatial video 
[Apple](https://www.apple.com)
  1. [Developer](https://developer.apple.com/)
  2. [ Documentation ](https://developer.apple.com/documentation/)


###  Platforms 
Toggle Menu 
  * [iOS](https://developer.apple.com/ios/)
  * [iPadOS](https://developer.apple.com/ipados/)
  * [macOS](https://developer.apple.com/macos/)
  * [tvOS](https://developer.apple.com/tvos/)
  * [visionOS](https://developer.apple.com/visionos/)
  * [watchOS](https://developer.apple.com/watchos/)


###  Tools 
Toggle Menu 
  * [Swift](https://developer.apple.com/swift/)
  * [SwiftUI](https://developer.apple.com/swiftui/)
  * [Swift Playground](https://developer.apple.com/swift-playground/)
  * [TestFlight](https://developer.apple.com/testflight/)
  * [Xcode](https://developer.apple.com/xcode/)
  * [Xcode Cloud](https://developer.apple.com/xcode-cloud/)
  * [SF Symbols](https://developer.apple.com/sf-symbols/)


###  Topics & Technologies 
Toggle Menu 
  * [Accessibility](https://developer.apple.com/accessibility/)
  * [Accessories](https://developer.apple.com/accessories/)
  * [App Extension](https://developer.apple.com/app-extensions/)
  * [App Store](https://developer.apple.com/app-store/)
  * [Audio & Video](https://developer.apple.com/audio/)
  * [Augmented Reality](https://developer.apple.com/augmented-reality/)
  * [Design](https://developer.apple.com/design/)
  * [Distribution](https://developer.apple.com/distribute/)
  * [Education](https://developer.apple.com/education/)
  * [Fonts](https://developer.apple.com/fonts/)
  * [Games](https://developer.apple.com/games/)
  * [Health & Fitness](https://developer.apple.com/health-fitness/)
  * [In-App Purchase](https://developer.apple.com/in-app-purchase/)
  * [Localization](https://developer.apple.com/localization/)
  * [Maps & Location](https://developer.apple.com/maps/)
  * [Machine Learning & AI](https://developer.apple.com/machine-learning/)
  * [Open Source](https://opensource.apple.com/)
  * [Security](https://developer.apple.com/security/)
  * [Safari & Web](https://developer.apple.com/safari/)


###  Resources 
Toggle Menu 
  *   * [Documentation](https://developer.apple.com/documentation/)
  * [Tutorials](https://developer.apple.com/learn/)
  * [Downloads](https://developer.apple.com/download/)
  * [Forums](https://developer.apple.com/forums/)
  * [Videos](https://developer.apple.com/videos/)


###  Support 
Toggle Menu 
  * [Support Articles](https://developer.apple.com/support/articles/)
  * [Contact Us](https://developer.apple.com/contact/)
  * [Bug Reporting](https://developer.apple.com/bug-reporting/)
  * [System Status](https://developer.apple.com/system-status/)


###  Account 
Toggle Menu 
  * [Apple Developer](https://developer.apple.com/account/)
  * [App Store Connect](https://appstoreconnect.apple.com/)
  * [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)
  * [Feedback Assistant](https://feedbackassistant.apple.com/)


###  Programs 
Toggle Menu 
  * [Apple Developer Program](https://developer.apple.com/programs/)
  * [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)
  * [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)
  * [MFi Program](https://mfi.apple.com/)
  * [News Partner Program](https://developer.apple.com/programs/news-partner/)
  * [Video Partner Program](https://developer.apple.com/programs/video-partner/)
  * [Security Bounty Program](https://developer.apple.com/security-bounty/)
  * [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)


###  Events 
Toggle Menu 
  * [Meet with Apple](https://developer.apple.com/events/)
  * [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)
  * [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)
  * [Apple Design Awards](https://developer.apple.com/design/awards/)
  * [Apple Developer Academies](https://developer.apple.com/academies/)
  * [WWDC](https://developer.apple.com/wwdc/)


To submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Favfoundation%2Fconverting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video).
Select a color scheme preference
Light
Dark
Auto
Copyright © 2025 [Apple Inc.](https://www.apple.com) All rights reserved. 
[ Terms of Use ](https://www.apple.com/legal/internet-services/terms/site.html)[ Privacy Policy ](https://www.apple.com/legal/privacy/)[ Agreements and Guidelines ](https://developer.apple.com/support/terms/)
