Source: https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml

[ Skip Navigation ](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#app-main)
  * [Global Nav Open Menu](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#ac-gn-menustate)[Global Nav Close Menu](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml)
  * [Apple Developer](https://developer.apple.com/)


[ Search Developer Cancel  ](https://developer.apple.com/search/)
  * [Apple Developer](https://developer.apple.com/)
  * [News](https://developer.apple.com/news/)
  * [Discover](https://developer.apple.com/discover/)
  * [Design](https://developer.apple.com/design/)
  * [Develop](https://developer.apple.com/develop/)
  * [Distribute](https://developer.apple.com/distribute/)
  * [Support](https://developer.apple.com/support/)
  * [Account](https://developer.apple.com/account/)
  * [](https://developer.apple.com/search/)


Cancel 
Only search within “Documentation”
### Quick Links
  * [Downloads](https://developer.apple.com/download/)
  * [Documentation](https://developer.apple.com/documentation/)
  * [Sample Code](https://developer.apple.com/documentation/samplecode/)
  * [Videos](https://developer.apple.com/videos/)
  * [Forums](https://developer.apple.com/forums/)

5 Quick Links
[ Documentation ](https://developer.apple.com/documentation)
[ Open Menu ](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml)
  * SwiftLanguage:  Swift  Objective-C 
Language: 
    * Swift 
    * [ Objective-C ](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml)


[](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml)
## [ Core ML  ](https://developer.apple.com/documentation/coreml)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 54 symbols inside <root> [MLCPUComputeDevice](https://developer.apple.com/documentation/coreml/mlcpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
18 of 54 symbols inside <root> [Downloading and Compiling a Model on the User’s Device](https://developer.apple.com/documentation/coreml/downloading-and-compiling-a-model-on-the-user-s-device)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
17 of 54 symbols inside <root>
App integration
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
18 of 54 symbols inside <root> [Downloading and Compiling a Model on the User’s Device](https://developer.apple.com/documentation/coreml/downloading-and-compiling-a-model-on-the-user-s-device)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
Collection
19 of 54 symbols inside <root> containing 9 symbols[Model Integration Samples](https://developer.apple.com/documentation/coreml/model-integration-samples)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
1 of 9 symbols inside 1345615585 
Tabular data models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
2 of 9 symbols inside 1345615585 [Integrating a Core ML Model into Your App](https://developer.apple.com/documentation/coreml/integrating-a-core-ml-model-into-your-app)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
3 of 9 symbols inside 1345615585 
Image classification models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
4 of 9 symbols inside 1345615585 [Using Core ML for semantic image segmentation](https://developer.apple.com/documentation/coreml/using-core-ml-for-semantic-image-segmentation)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
5 of 9 symbols inside 1345615585 [Classifying Images with Vision and Core ML](https://developer.apple.com/documentation/coreml/classifying-images-with-vision-and-core-ml)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
6 of 9 symbols inside 1345615585 [Detecting human body poses in an image](https://developer.apple.com/documentation/coreml/detecting-human-body-poses-in-an-image)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
7 of 9 symbols inside 1345615585 [Understanding a Dice Roll with Vision and Object Detection](https://developer.apple.com/documentation/coreml/understanding-a-dice-roll-with-vision-and-object-detection)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
8 of 9 symbols inside 1345615585 
Text classification models
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
9 of 9 symbols inside 1345615585 [Finding answers to questions in a text document](https://developer.apple.com/documentation/coreml/finding-answers-to-questions-in-a-text-document)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
20 of 54 symbols inside <root>
Model encryption
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
21 of 54 symbols inside <root> [Generating a Model Encryption Key](https://developer.apple.com/documentation/coreml/generating-a-model-encryption-key)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
22 of 54 symbols inside <root> [Encrypting a Model in Your App](https://developer.apple.com/documentation/coreml/encrypting-a-model-in-your-app)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
23 of 54 symbols inside <root>
Compute devices
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
E
24 of 54 symbols inside <root> containing 6 symbols[MLComputeDevice](https://developer.apple.com/documentation/coreml/mlcomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
25 of 54 symbols inside <root> [MLCPUComputeDevice](https://developer.apple.com/documentation/coreml/mlcpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
26 of 54 symbols inside <root> containing 2 symbols[MLGPUComputeDevice](https://developer.apple.com/documentation/coreml/mlgpucomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
27 of 54 symbols inside <root> containing 2 symbols[MLNeuralEngineComputeDevice](https://developer.apple.com/documentation/coreml/mlneuralenginecomputedevice)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
rP
28 of 54 symbols inside <root> [MLComputeDeviceProtocol](https://developer.apple.com/documentation/coreml/mlcomputedeviceprotocol)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
29 of 54 symbols inside <root>
Compute plan
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
C
30 of 54 symbols inside <root> containing 13 symbols[MLComputePlan](https://developer.apple.com/documentation/coreml/mlcomputeplan-1w21n)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
E
31 of 54 symbols inside <root> containing 11 symbols[MLModelStructure](https://developer.apple.com/documentation/coreml/mlmodelstructure-swift.enum)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
32 of 54 symbols inside <root> containing 8 symbols[MLComputePolicy](https://developer.apple.com/documentation/coreml/mlcomputepolicy)
63 items were found. Tab back to navigate through them. 
/ 
Navigator is ready 
  * [ Core ML ](https://developer.apple.com/documentation/coreml)
  * [ Model Integration Samples ](https://developer.apple.com/documentation/coreml/model-integration-samples)
  * [ Classifying Images with Vision and Core ML ](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml)
  *     * [ Model Integration Samples ](https://developer.apple.com/documentation/coreml/model-integration-samples)
    * Classifying Images with Vision and Core ML 


Sample Code
# Classifying Images with Vision and Core ML
Crop and scale photos using the Vision framework and classify them with a Core ML model.
[ Download ](https://docs-assets.developer.apple.com/published/bd38aaabfb4e/ClassifyingImagesWithVisionAndCoreML.zip)
iOS 14.0+iPadOS 14.0+Xcode 13.4+iPad 14.0+
## [Overview](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Overview)
The app in this sample identifies the most prominent object in an image by using MobileNet, an open source image classifier model that recognizes around 1,000 different categories.
![Screenshots of the app identifying a monarch butterfly, broccoli, and a daisy in a field.](https://docs-assets.developer.apple.com/published/a6c71d92d5585738a6b20c2a3d6c99c1/Screenshots%402x~dark.png)
Each time a user selects a photo from the library or takes a photo with a camera, the app passes it to a [Vision](https://developer.apple.com/documentation/Vision) image classification request. Vision resizes and crops the photo to meet the MobileNet model’s constraints for its image input, and then passes the photo to the model using the [Core ML](https://developer.apple.com/documentation/CoreML) framework behind the scenes. Once the model generates a prediction, Vision relays it back to the app, which presents the results to the user.
The sample uses MobileNet as an example of how to use a third-party Core ML model. You can download open source models — including a newer version of MobileNet — on the [Core ML model gallery](https://developer.apple.com/machine-learning/models).
Before you integrate a third-party model to solve a problem — which may increase the size of your app — consider using an API in the SDK. For example, the [Vision](https://developer.apple.com/documentation/Vision) framework’s [VNClassifyImageRequest](https://developer.apple.com/documentation/Vision/VNClassifyImageRequest) class offers the same functionality as MobileNet, but with potentially better performance and without increasing the size of your app (see [Classifying Images for Categorization and Search](https://developer.apple.com/documentation/Vision/classifying-images-for-categorization-and-search)).
Note
You can make a custom image classifier that identifies your choice of object types with [Create ML](https://developer.apple.com/documentation/CreateML). See [Creating an Image Classifier Model](https://developer.apple.com/documentation/CreateML/creating-an-image-classifier-model) to learn how to create a custom image classifier that can replace the MobileNet model in this sample.
### [Configure the sample code project](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Configure-the-sample-code-project)
The sample targets iOS 14 or later, but the MobileNet model in the project works with:
  * iOS 11 or later
  * macOS 10.13 or later


To take photos within the app, run the sample on a device with a camera. Otherwise, you can select photos from the library in Simulator.
Note
Add your own photos to the photo library in Simulator by dragging photos onto its window.
### [Create an image classifier instance](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Create-an-image-classifier-instance)
At launch, the `ImagePredictor` class creates an image classifier singleton by calling its `createImageClassifier()` type method.
The method creates a Core ML model instance for Vision by:
  1. Creating an instance of the model’s wrapper class that Xcode auto-generates at compile time
  2. Retrieving the wrapper class instance’s underlying [‘MLModel’](https://developer.apple.com/documentation/CoreML/MLModel) property
  3. Passing the model instance to a [`VNCoreMLModel`](https://developer.apple.com/documentation/Vision/VNCoreMLModel) initializer


The Image Predictor class minimizes runtime by only creating a single instance it shares across the app.
Note
Share a single [`VNCoreMLModel`](https://developer.apple.com/documentation/Vision/VNCoreMLModel) instance for each Core ML model in your project.
### [Create an image classification request](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Create-an-image-classification-request)
The Image Predictor class creates an image classification request — a [`VNCoreMLRequest`](https://developer.apple.com/documentation/Vision/VNCoreMLRequest) instance — by passing the shared image classifier model instance and a request handler to its initializer.
The method tells Vision how to adjust images that don’t meet the model’s image input constraints by setting the request’s [`imageCropAndScaleOption`](https://developer.apple.com/documentation/Vision/VNCoreMLRequest/imageCropAndScaleOption) property to [`centerCrop`](https://developer.apple.com/documentation/Vision/VNImageCropAndScaleOption/centerCrop).
### [Create a request handler](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Create-a-request-handler)
The Image Predictor’s `makePredictions(for photo, ...)` method creates a [`VNImageRequestHandler`](https://developer.apple.com/documentation/Vision/VNImageRequestHandler) for each image by passing the image and its orientation to the initializer.
Vision rotates the image based on `orientation` — a [`CGImagePropertyOrientation`](https://developer.apple.com/documentation/ImageIO/CGImagePropertyOrientation) instance — before sending the image to the model.
If the image you want to classify has a URL, create a Vision image request handler with one of these initializers:
  * [`VNImageRequestHandler(url:options:)`](https://developer.apple.com/documentation/Vision/VNImageRequestHandler/init\(url:options:\))
  * [`VNImageRequestHandler(url:orientation:options:)`](https://developer.apple.com/documentation/Vision/VNImageRequestHandler/init\(url:orientation:options:\))


### [Start the Request](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Start-the-Request)
The [`makePredictions(for photo, ...)`][makePredictions] method starts the request by adding it into a [`VNRequest`](https://developer.apple.com/documentation/Vision/VNRequest) array and passes it to the handler’s [`perform(_:)`](https://developer.apple.com/documentation/Vision/VNImageRequestHandler/perform\(_:\)) method.
Note
You can perform multiple Vision requests on the same image by adding each request to the array you pass to the [`perform(_:)`](https://developer.apple.com/documentation/Vision/VNImageRequestHandler/perform\(_:\)) method’s `requests` parameter.
### [Retrieve the request’s results](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Retrieve-the-requests-results)
When the image classification request is finished, Vision notifies the Image Predictor by calling the request’s completion handler, `visionRequestHandler(_:error:)`. The method retrieves the request’s [`results`](https://developer.apple.com/documentation/Vision/VNRequest/results) by:
  1. Checking the `error` parameter
  2. Casting `results` to a `VNClassificationObservation` array


The Image Predictor converts each result to `Prediction` instances, a simple structure with two string properties.
The method sends the `predictions` array to the Image Predictor’s client — the main view controller — by calling the client’s completion handler.
### [Format and present the predictions](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Format-and-present-the-predictions)
The main view controller’s `imagePredictionHandler(_:)` method formats the individual predictions into a single string and updates a label in the app’s UI using helper methods.
The `updatePredictionLabel(_:)` helper method safely updates the UI by updating the label’s text on the main dispatch queue.
Important
Keep your app’s UI responsive by making predictions with Core ML models off of the main thread.
## [See Also](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#see-also)
### [Image classification models](https://developer.apple.com/documentation/CoreML/classifying-images-with-vision-and-core-ml#Image-classification-models)
[Using Core ML for semantic image segmentation](https://developer.apple.com/documentation/coreml/using-core-ml-for-semantic-image-segmentation)
Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.
[Detecting human body poses in an image](https://developer.apple.com/documentation/coreml/detecting-human-body-poses-in-an-image)
Locate people and the stance of their bodies by analyzing an image with a PoseNet model.
[Understanding a Dice Roll with Vision and Object Detection](https://developer.apple.com/documentation/coreml/understanding-a-dice-roll-with-vision-and-object-detection)
Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.
Current page is Classifying Images with Vision and Core ML 
[Apple](https://www.apple.com)
  1. [Developer](https://developer.apple.com/)
  2. [ Documentation ](https://developer.apple.com/documentation/)


###  Platforms 
Toggle Menu 
  * [iOS](https://developer.apple.com/ios/)
  * [iPadOS](https://developer.apple.com/ipados/)
  * [macOS](https://developer.apple.com/macos/)
  * [tvOS](https://developer.apple.com/tvos/)
  * [visionOS](https://developer.apple.com/visionos/)
  * [watchOS](https://developer.apple.com/watchos/)


###  Tools 
Toggle Menu 
  * [Swift](https://developer.apple.com/swift/)
  * [SwiftUI](https://developer.apple.com/swiftui/)
  * [Swift Playground](https://developer.apple.com/swift-playground/)
  * [TestFlight](https://developer.apple.com/testflight/)
  * [Xcode](https://developer.apple.com/xcode/)
  * [Xcode Cloud](https://developer.apple.com/xcode-cloud/)
  * [SF Symbols](https://developer.apple.com/sf-symbols/)


###  Topics & Technologies 
Toggle Menu 
  * [Accessibility](https://developer.apple.com/accessibility/)
  * [Accessories](https://developer.apple.com/accessories/)
  * [App Extension](https://developer.apple.com/app-extensions/)
  * [App Store](https://developer.apple.com/app-store/)
  * [Audio & Video](https://developer.apple.com/audio/)
  * [Augmented Reality](https://developer.apple.com/augmented-reality/)
  * [Design](https://developer.apple.com/design/)
  * [Distribution](https://developer.apple.com/distribute/)
  * [Education](https://developer.apple.com/education/)
  * [Fonts](https://developer.apple.com/fonts/)
  * [Games](https://developer.apple.com/games/)
  * [Health & Fitness](https://developer.apple.com/health-fitness/)
  * [In-App Purchase](https://developer.apple.com/in-app-purchase/)
  * [Localization](https://developer.apple.com/localization/)
  * [Maps & Location](https://developer.apple.com/maps/)
  * [Machine Learning & AI](https://developer.apple.com/machine-learning/)
  * [Open Source](https://opensource.apple.com/)
  * [Security](https://developer.apple.com/security/)
  * [Safari & Web](https://developer.apple.com/safari/)


###  Resources 
Toggle Menu 
  *   * [Documentation](https://developer.apple.com/documentation/)
  * [Tutorials](https://developer.apple.com/learn/)
  * [Downloads](https://developer.apple.com/download/)
  * [Forums](https://developer.apple.com/forums/)
  * [Videos](https://developer.apple.com/videos/)


###  Support 
Toggle Menu 
  * [Support Articles](https://developer.apple.com/support/articles/)
  * [Contact Us](https://developer.apple.com/contact/)
  * [Bug Reporting](https://developer.apple.com/bug-reporting/)
  * [System Status](https://developer.apple.com/system-status/)


###  Account 
Toggle Menu 
  * [Apple Developer](https://developer.apple.com/account/)
  * [App Store Connect](https://appstoreconnect.apple.com/)
  * [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)
  * [Feedback Assistant](https://feedbackassistant.apple.com/)


###  Programs 
Toggle Menu 
  * [Apple Developer Program](https://developer.apple.com/programs/)
  * [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)
  * [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)
  * [MFi Program](https://mfi.apple.com/)
  * [News Partner Program](https://developer.apple.com/programs/news-partner/)
  * [Video Partner Program](https://developer.apple.com/programs/video-partner/)
  * [Security Bounty Program](https://developer.apple.com/security-bounty/)
  * [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)


###  Events 
Toggle Menu 
  * [Meet with Apple](https://developer.apple.com/events/)
  * [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)
  * [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)
  * [Apple Design Awards](https://developer.apple.com/design/awards/)
  * [Apple Developer Academies](https://developer.apple.com/academies/)
  * [WWDC](https://developer.apple.com/wwdc/)


To submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2FCoreML%2Fclassifying-images-with-vision-and-core-ml).
Select a color scheme preference
Light
Dark
Auto
Copyright © 2025 [Apple Inc.](https://www.apple.com) All rights reserved. 
[ Terms of Use ](https://www.apple.com/legal/internet-services/terms/site.html)[ Privacy Policy ](https://www.apple.com/legal/privacy/)[ Agreements and Guidelines ](https://developer.apple.com/support/terms/)
