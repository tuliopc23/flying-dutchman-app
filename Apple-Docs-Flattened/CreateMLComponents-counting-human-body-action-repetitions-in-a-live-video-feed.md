Source: https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed

[ Skip Navigation ](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#app-main)
  * [Global Nav Open Menu](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#ac-gn-menustate)[Global Nav Close Menu](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)
  * [Apple Developer](https://developer.apple.com/)


[ Search Developer Cancel  ](https://developer.apple.com/search/)
  * [Apple Developer](https://developer.apple.com/)
  * [News](https://developer.apple.com/news/)
  * [Discover](https://developer.apple.com/discover/)
  * [Design](https://developer.apple.com/design/)
  * [Develop](https://developer.apple.com/develop/)
  * [Distribute](https://developer.apple.com/distribute/)
  * [Support](https://developer.apple.com/support/)
  * [Account](https://developer.apple.com/account/)
  * [](https://developer.apple.com/search/)


Cancel 
Only search within “Documentation”
### Quick Links
  * [Downloads](https://developer.apple.com/download/)
  * [Documentation](https://developer.apple.com/documentation/)
  * [Sample Code](https://developer.apple.com/documentation/samplecode/)
  * [Videos](https://developer.apple.com/videos/)
  * [Forums](https://developer.apple.com/forums/)

5 Quick Links
[ Documentation ](https://developer.apple.com/documentation)
[ Open Menu ](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)
  * SwiftLanguage:  Swift  Objective-C 
Language: 
    * Swift 
    * [ Objective-C ](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)


[](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)
## [ Create ML Components  ](https://developer.apple.com/documentation/createmlcomponents)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
28 of 217 symbols inside <root>
Audio components
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
10 of 217 symbols inside <root> containing 9 symbols[ImageColorTransformer](https://developer.apple.com/documentation/createmlcomponents/imagecolortransformer)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
11 of 217 symbols inside <root> containing 6 symbols[ImageExposureAdjuster](https://developer.apple.com/documentation/createmlcomponents/imageexposureadjuster)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
12 of 217 symbols inside <root> containing 7 symbols[ImageFlipper](https://developer.apple.com/documentation/createmlcomponents/imageflipper)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
13 of 217 symbols inside <root> containing 6 symbols[ImageRotator](https://developer.apple.com/documentation/createmlcomponents/imagerotator)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
14 of 217 symbols inside <root> containing 6 symbols[RandomImageNoiseGenerator](https://developer.apple.com/documentation/createmlcomponents/randomimagenoisegenerator)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
15 of 217 symbols inside <root> containing 10 symbols[MLModelImageFeatureExtractor](https://developer.apple.com/documentation/createmlcomponents/mlmodelimagefeatureextractor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
16 of 217 symbols inside <root>
Pose components
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
17 of 217 symbols inside <root> [Counting human body action repetitions in a live video feed](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
18 of 217 symbols inside <root> containing 10 symbols[Pose](https://developer.apple.com/documentation/createmlcomponents/pose)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
19 of 217 symbols inside <root> containing 51 symbols[JointKey](https://developer.apple.com/documentation/createmlcomponents/jointkey)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
20 of 217 symbols inside <root> containing 8 symbols[JointPoint](https://developer.apple.com/documentation/createmlcomponents/jointpoint)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
21 of 217 symbols inside <root> containing 9 symbols[PoseSelector](https://developer.apple.com/documentation/createmlcomponents/poseselector)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
E
22 of 217 symbols inside <root> containing 6 symbols[PoseSelectionStrategy](https://developer.apple.com/documentation/createmlcomponents/poseselectionstrategy)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
23 of 217 symbols inside <root> containing 8 symbols[JointsSelector](https://developer.apple.com/documentation/createmlcomponents/jointsselector)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
24 of 217 symbols inside <root> containing 4 symbols[HumanBodyPoseExtractor](https://developer.apple.com/documentation/createmlcomponents/humanbodyposeextractor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
25 of 217 symbols inside <root> containing 4 symbols[HumanHandPoseExtractor](https://developer.apple.com/documentation/createmlcomponents/humanhandposeextractor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
26 of 217 symbols inside <root> containing 5 symbols[HumanBodyActionCounter](https://developer.apple.com/documentation/createmlcomponents/humanbodyactioncounter)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
27 of 217 symbols inside <root> containing 5 symbols[HumanBodyActionPeriodPredictor](https://developer.apple.com/documentation/createmlcomponents/humanbodyactionperiodpredictor)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
28 of 217 symbols inside <root>
Audio components
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
29 of 217 symbols inside <root> containing 14 symbols[AudioReader](https://developer.apple.com/documentation/createmlcomponents/audioreader)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
30 of 217 symbols inside <root> containing 8 symbols[AudioFeaturePrint](https://developer.apple.com/documentation/createmlcomponents/audiofeatureprint)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
S
31 of 217 symbols inside <root> containing 6 symbols[AudioConvertingTransformer](https://developer.apple.com/documentation/createmlcomponents/audioconvertingtransformer)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
32 of 217 symbols inside <root>
Time-based components
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
33 of 217 symbols inside <root> [Creating a time-series classifier](https://developer.apple.com/documentation/createmlcomponents/creating-a-time-series-classifier)
To navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow 
34 of 217 symbols inside <root> [Creating a time-series forecaster](https://developer.apple.com/documentation/createmlcomponents/creating-a-time-series-forecaster)
217 items were found. Tab back to navigate through them. 
/ 
Navigator is ready 
  * [ Create ML Components ](https://developer.apple.com/documentation/createmlcomponents)
  * [ Counting human body action repetitions in a live video feed ](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed)
  *     * Counting human body action repetitions in a live video feed 


Sample Code
# Counting human body action repetitions in a live video feed
Use Create ML Components to analyze a series of video frames and count a person’s repetitive or periodic body movements.
[ Download ](https://docs-assets.developer.apple.com/published/6320e9d157f9/CountingHumanBodyActionRepetitionsInALiveVideoFeed.zip)
iOS 16.0+iPadOS 16.0+Xcode 14.0+
## [Overview](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Overview)
This sample app counts a person’s repetitive or periodic body movements (_actions_) by analyzing a series of video frames and making a prediction with a human body action repetition counter. The counter in this sample can count arbitrary body moves that occur at moderate speed, such as jumping jacks, dance spins, and waving arms.
![A flow diagram that illustrates two people performing jumping jacks in front](https://docs-assets.developer.apple.com/published/e1ad4893d09c542febf69ba875fcad1a/createml-components-framework-overview%402x.png)
The app continually presents the current action repetition count on top of a live, full-screen video feed from the camera in portrait orientation. When the app detects one or more people in the frame, it overlays a wireframe body pose on each person. At the same time, the app predicts the action repetition count about the most prominent person across multiple frames, typically whoever is closest to the camera.
The app begins by configuring a camera to generate video frames, then directs the frames through a series of transformers it chains together with [Create ML Components](https://developer.apple.com/documentation/createmlcomponents). These methods work together to:
  1. Read camera frames in real time using [`VideoReader`](https://developer.apple.com/documentation/createmlcomponents/videoreader).
  2. Analyze each frame to locate any human body poses using [`HumanBodyPoseExtractor`](https://developer.apple.com/documentation/createmlcomponents/humanbodyposeextractor), and redirect the pose stream with an [AsyncChannel](https://github.com/apple/swift-async-algorithms/blob/main/Sources/AsyncAlgorithms/AsyncAlgorithms.docc/Guides/Channel.md) to allow multiple consumers.
  3. Optionally, downsample the stream using a [`Downsampler`](https://developer.apple.com/documentation/createmlcomponents/downsampler) to process the observed actions in different speeds. To improve performance, you can move the downsampler to an earlier stage in the pipeline if you don’t need to render poses on every frame.
  4. Isolate the prominent pose using [`PoseSelector`](https://developer.apple.com/documentation/createmlcomponents/poseselector).
  5. Optionally, use [`JointsSelector`](https://developer.apple.com/documentation/createmlcomponents/jointsselector) to select only joints of interest for counting.
  6. Aggregate the prominent pose’s position data over time using [`SlidingWindowTransformer`](https://developer.apple.com/documentation/createmlcomponents/slidingwindowtransformer).
  7. Predict action repetitions by sending aggregate data to the [`HumanBodyActionCounter`](https://developer.apple.com/documentation/createmlcomponents/humanbodyactioncounter).


Note
This sample code project is associated with WWDC22 session [110332: What’s new in Create ML](https://developer.apple.com/wwdc22/110332/).
## [Configure the sample code project](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Configure-the-sample-code-project)
This sample code project requires a device with iOS 16 or later, or iPadOS 16 or later. To build this project:
  1. Double-click the `CountMyActions.xcodeproj` project to open it in Xcode.
  2. In Xcode, from the Project navigator, select the `CountMyActions` project and click the Signing & Capabilities tab.
  3. Select your development team from the Add Account pop-up menu.
  4. Select your target device from the scheme menu, and choose Product > Run.


## [Start a live video feed](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Start-a-live-video-feed)
The app uses [`VideoReader`](https://developer.apple.com/documentation/createmlcomponents/videoreader) to configure the device’s camera and generate an asynchronous video frame sequence. The [`VideoReader.CameraConfiguration`](https://developer.apple.com/documentation/createmlcomponents/videoreader/cameraconfiguration) specifies the front- or rear-facing camera, and configures its pixel format and resolution. This app supports portrait orientation only. Low lighting and other factors can vary the frame rate, which may affect the counting performance, so ensure the person’s full body is visible in bright environments.
```
/// The camera configuration to define the basic camera position, pixel format, and resolution to use.
private var configuration = VideoReader.CameraConfiguration()

```

When the app first launches — or when the user toggles the camera — the video reader configures a camera device, starts the video-processing pipeline, and produces a frame sequence output with [`readCamera(configuration:)`](https://developer.apple.com/documentation/createmlcomponents/videoreader/readcamera\(configuration:\)).
```
/// Start the video-processing pipeline by displaying the poses in the camera frames and
/// starting the action repetition count prediction stream.
func startVideoProcessingPipeline() {


    if let displayCameraTask = displayCameraTask {
        displayCameraTask.cancel()
    }


    displayCameraTask = Task {
        // Display poses on top of each camera frame.
        try await self.displayPoseInCamera()
    }


    if predictionTask == nil {
        predictionTask = Task {
            // Predict the action repetition count.
            try await self.predictCount()
        }
    }
}

```

## [Analyze each frame for body poses](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Analyze-each-frame-for-body-poses)
The [`HumanBodyPoseExtractor`](https://developer.apple.com/documentation/createmlcomponents/humanbodyposeextractor) is a transformer that can locate any human body poses from an image or a video frame.
```
/// A Create ML Components transformer to extract human body poses from a single image or a video frame.
private let poseExtractor = HumanBodyPoseExtractor()

```

When the transformation completes, the method creates and returns a [`Pose`](https://developer.apple.com/documentation/createmlcomponents/pose) array that contains one pose for every detected person in the same frame.
```
// Extract poses in every frame.
let poses = try await poseExtractor.applied(to: frame.feature)

```

The `Pose` structure serves the following purposes:
  * Calculates the pose’s area within a frame (See the “Isolate a body pose” section below.).
  * Draws each detected pose as a wireframe of points and lines (See the “Present the poses to the user” section below.).


For more information about the underlying human body pose model, see [Detecting Human Body Poses in Images](https://developer.apple.com/documentation/Vision/detecting-human-body-poses-in-images).
## [Create a pose stream](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Create-a-pose-stream)
[AsyncChannel](https://github.com/apple/swift-async-algorithms/blob/main/Sources/AsyncAlgorithms/AsyncAlgorithms.docc/Guides/Channel.md) sends the extracted poses to a separate asynchronous stream. This allows additional consumers to obtain poses from the upstream asynchronous sequence. `AsyncChannel` requires the inclusion of the [AsyncAlgorithms](https://github.com/apple/swift-async-algorithms) Swift package.
```
/// An asynchronous channel to divert the pose stream for another consumer.
private let poseStream = AsyncChannel<TemporalFeature<[Pose]>>()

```

## [Create an action repetition counting pipeline](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Create-an-action-repetition-counting-pipeline)
The `ActionCounter` structure consists of a pipeline of [Create ML Components](https://developer.apple.com/documentation/createmlcomponents) transformers to achieve continuous action repetition counting. It takes a pose stream as input and returns an asynchronous sequence of cumulative counts.
```
/// The counter to count action repetitions from a pose stream.
private let actionCounter = ActionCounter()

```

## [Downsample a pose stream](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Downsample-a-pose-stream)
The first optional transformer in the pipeline, [`Downsampler`](https://developer.apple.com/documentation/createmlcomponents/downsampler), downsamples the incoming pose sequence by an integer factor. This allows the pipeline to process and count much slower actions. For example, without downsampling, the original counter model can handle moderate speed actions, about one repetition per second, such as jumping jacks. A downsampling factor of three can effectively speed up slower actions, such as pushups or a complex dance sequence with about one repetition per 3 seconds, and still allow the model to count the actions.
```
// Use an optional Downsampler transformer to downsample the
// incoming frames (that is, effectively speed up the observed actions).
let pipeline = Downsampler(factor: 1)

```

## [Isolate a body pose](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Isolate-a-body-pose)
The next transformer in the pipeline, [`PoseSelector`](https://developer.apple.com/documentation/createmlcomponents/poseselector), selects a single pose from the array of poses by using the default strategy, namely, selecting the most prominent person by their maximum bounding box area.
```
// Use a PoseSelector transformer to select one pose to count if
// the system detects multiple poses.
    .appending(PoseSelector(strategy: .maximumBoundingBoxArea))

```

The goal of this strategy is to consistently select the same person’s pose from a crowd over time.
![A flow diagram that illustrates two detected poses in the same frame passing](https://docs-assets.developer.apple.com/published/530dc013c2438389999ae0bae4d672e6/pose-selector%402x.png)
Important
Get the most accurate predictions by using whatever strategy best tracks a person from frame to frame.
## [Select a subset of body joints](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Select-a-subset-of-body-joints)
The next optional transformer in the pipeline, [`JointsSelector`](https://developer.apple.com/documentation/createmlcomponents/jointsselector), selects or ignores a specified subset of body joints from the pose.
![A flow diagram that illustrates a full body pose passing into a joints](https://docs-assets.developer.apple.com/published/d014f86d1062d31d77ab37f14103864d/joints-selector%402x.png)
For example, to count only upper-body movements, the transformer can ignore lower-body joints in the pose, such as knees and ankles, which can eliminate noise by ignoring any leg movements.
```
// Use an optional JointsSelector transformer to specifically ignore
// or select a set of joints in a pose to include in counting.
    .appending(JointsSelector(ignoredJoints: [.nose, .leftEye, .leftEar, .rightEye, .rightEar]))

```

## [Gather a window of poses](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Gather-a-window-of-poses)
The next transformer in the pipeline is a [`SlidingWindowTransformer`](https://developer.apple.com/documentation/createmlcomponents/slidingwindowtransformer) that receives a pose sequence from its upstream and gathers the frames into an array by providing the following parameters:
  * A `stride` that determines the number of frames to count before updating the pose window
  * A `length` that determines the window size, namely, how many frames to group together


```
// Use a SlidingWindowTransformer to group frames into windows, and
// prepare them for prediction.
    .appending(SlidingWindowTransformer<Pose>(stride: 5, length: 90))

```

![A flow diagram that illustrates a pose sequence passing into a sliding window](https://docs-assets.developer.apple.com/published/ee43eb3b50c15b7cd790f68415809356/sliding-window%402x.png)
The action repetition counter assumes a fixed length of 90, where the sliding window transformer groups 90 frames together to generate a single prediction count. The stride is adjustable. An example is a stride of 10 frames, indicating the count updates every 10 frames, which is about 0.3 seconds if the frame rate is 30 frames per second. When the stride is smaller than the length, the windows overlap.
## [Predict the person’s action repetition count](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Predict-the-persons-action-repetition-count)
The next transformer in the pipeline, [`HumanBodyActionCounter`](https://developer.apple.com/documentation/createmlcomponents/humanbodyactioncounter), takes a stream of grouped pose windows as input and produces a [`HumanBodyActionCounter.CumulativeSumSequence`](https://developer.apple.com/documentation/createmlcomponents/humanbodyactioncounter/cumulativesumsequence) where each result is a cumulative count of the actions in the sequence. Live counting occurs by iterating each item in the resulted sequence.
```
// Use a HumanBodyActionCounter transformer to count actions from
// each window and produce cumulative counts for the input stream.
    .appending(HumanBodyActionCounter())

```

## [Present the count to the user](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Present-the-count-to-the-user)
The final count appears as a [SwiftUI](https://developer.apple.com/documentation/SwiftUI) label on the screen using the `OverlayView` structure on the main thread.
## [Present the poses to the user](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Present-the-poses-to-the-user)
The app visualizes the result of each detected human body pose by drawing the poses on top of the frame that [`HumanBodyPoseExtractor`](https://developer.apple.com/documentation/createmlcomponents/humanbodyposeextractor) finds them in. Each time the `poseExtractor` creates an array of [`Pose`](https://developer.apple.com/documentation/createmlcomponents/pose) instances, the `PosesView` iterates each detected pose and draws it by calling its `drawWireframe(to:applying:)` method, which draws the pose as a wireframe of connection lines and joint circles.
```
// Draw all the poses Vision finds in the frame.
for pose in poses {
    // Draw each pose as a wireframe at the scale of the image.
    pose.drawWireframe(to: context, applying: pointTransform)
}

```

The `ViewModel` presents the image and poses onscreen by calling `display(image:, poses:)` method.
## [See Also](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#see-also)
### [Pose components](https://developer.apple.com/documentation/createmlcomponents/counting-human-body-action-repetitions-in-a-live-video-feed#Pose-components)
[`struct Pose`](https://developer.apple.com/documentation/createmlcomponents/pose)
A pose that contains joint keypoints from a person, a hand, or a combination.
[`struct JointKey`](https://developer.apple.com/documentation/createmlcomponents/jointkey)
A key that uniquely identifies a joint.
[`struct JointPoint`](https://developer.apple.com/documentation/createmlcomponents/jointpoint)
A joint in a pose that contains a location and scoring information.
[`struct PoseSelector`](https://developer.apple.com/documentation/createmlcomponents/poseselector)
A transformer that selects one pose from an array of poses.
[`enum PoseSelectionStrategy`](https://developer.apple.com/documentation/createmlcomponents/poseselectionstrategy)
Pose selection strategy.
[`struct JointsSelector`](https://developer.apple.com/documentation/createmlcomponents/jointsselector)
Joints selector from a pose.
[`struct HumanBodyPoseExtractor`](https://developer.apple.com/documentation/createmlcomponents/humanbodyposeextractor)
The human body pose image feature extractor.
[`struct HumanHandPoseExtractor`](https://developer.apple.com/documentation/createmlcomponents/humanhandposeextractor)
The human hand pose image feature extractor.
[`struct HumanBodyActionCounter`](https://developer.apple.com/documentation/createmlcomponents/humanbodyactioncounter)
A human body action repetition counting transformer that takes window of human body poses and produces cumulative human body action repetition counts.
[`struct HumanBodyActionPeriodPredictor`](https://developer.apple.com/documentation/createmlcomponents/humanbodyactionperiodpredictor)
A human body action period predictor transformer that takes window of poses and produces a window of predictions.
Current page is Counting human body action repetitions in a live video feed 
[Apple](https://www.apple.com)
  1. [Developer](https://developer.apple.com/)
  2. [ Documentation ](https://developer.apple.com/documentation/)


###  Platforms 
Toggle Menu 
  * [iOS](https://developer.apple.com/ios/)
  * [iPadOS](https://developer.apple.com/ipados/)
  * [macOS](https://developer.apple.com/macos/)
  * [tvOS](https://developer.apple.com/tvos/)
  * [visionOS](https://developer.apple.com/visionos/)
  * [watchOS](https://developer.apple.com/watchos/)


###  Tools 
Toggle Menu 
  * [Swift](https://developer.apple.com/swift/)
  * [SwiftUI](https://developer.apple.com/swiftui/)
  * [Swift Playground](https://developer.apple.com/swift-playground/)
  * [TestFlight](https://developer.apple.com/testflight/)
  * [Xcode](https://developer.apple.com/xcode/)
  * [Xcode Cloud](https://developer.apple.com/xcode-cloud/)
  * [SF Symbols](https://developer.apple.com/sf-symbols/)


###  Topics & Technologies 
Toggle Menu 
  * [Accessibility](https://developer.apple.com/accessibility/)
  * [Accessories](https://developer.apple.com/accessories/)
  * [App Extension](https://developer.apple.com/app-extensions/)
  * [App Store](https://developer.apple.com/app-store/)
  * [Audio & Video](https://developer.apple.com/audio/)
  * [Augmented Reality](https://developer.apple.com/augmented-reality/)
  * [Design](https://developer.apple.com/design/)
  * [Distribution](https://developer.apple.com/distribute/)
  * [Education](https://developer.apple.com/education/)
  * [Fonts](https://developer.apple.com/fonts/)
  * [Games](https://developer.apple.com/games/)
  * [Health & Fitness](https://developer.apple.com/health-fitness/)
  * [In-App Purchase](https://developer.apple.com/in-app-purchase/)
  * [Localization](https://developer.apple.com/localization/)
  * [Maps & Location](https://developer.apple.com/maps/)
  * [Machine Learning & AI](https://developer.apple.com/machine-learning/)
  * [Open Source](https://opensource.apple.com/)
  * [Security](https://developer.apple.com/security/)
  * [Safari & Web](https://developer.apple.com/safari/)


###  Resources 
Toggle Menu 
  *   * [Documentation](https://developer.apple.com/documentation/)
  * [Tutorials](https://developer.apple.com/learn/)
  * [Downloads](https://developer.apple.com/download/)
  * [Forums](https://developer.apple.com/forums/)
  * [Videos](https://developer.apple.com/videos/)


###  Support 
Toggle Menu 
  * [Support Articles](https://developer.apple.com/support/articles/)
  * [Contact Us](https://developer.apple.com/contact/)
  * [Bug Reporting](https://developer.apple.com/bug-reporting/)
  * [System Status](https://developer.apple.com/system-status/)


###  Account 
Toggle Menu 
  * [Apple Developer](https://developer.apple.com/account/)
  * [App Store Connect](https://appstoreconnect.apple.com/)
  * [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)
  * [Feedback Assistant](https://feedbackassistant.apple.com/)


###  Programs 
Toggle Menu 
  * [Apple Developer Program](https://developer.apple.com/programs/)
  * [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)
  * [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)
  * [MFi Program](https://mfi.apple.com/)
  * [News Partner Program](https://developer.apple.com/programs/news-partner/)
  * [Video Partner Program](https://developer.apple.com/programs/video-partner/)
  * [Security Bounty Program](https://developer.apple.com/security-bounty/)
  * [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)


###  Events 
Toggle Menu 
  * [Meet with Apple](https://developer.apple.com/events/)
  * [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)
  * [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)
  * [Apple Design Awards](https://developer.apple.com/design/awards/)
  * [Apple Developer Academies](https://developer.apple.com/academies/)
  * [WWDC](https://developer.apple.com/wwdc/)


To submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fcreatemlcomponents%2Fcounting-human-body-action-repetitions-in-a-live-video-feed).
Select a color scheme preference
Light
Dark
Auto
Copyright © 2025 [Apple Inc.](https://www.apple.com) All rights reserved. 
[ Terms of Use ](https://www.apple.com/legal/internet-services/terms/site.html)[ Privacy Policy ](https://www.apple.com/legal/privacy/)[ Agreements and Guidelines ](https://developer.apple.com/support/terms/)
